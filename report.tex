\documentclass{article}\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\usepackage{graphicx}
% Set page margins
\usepackage{geometry}
\geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
% Remove paragraph indenting
\setlength\parindent{0pt}
% Add hyperlinks
\usepackage{hyperref}
% Use math equations
\usepackage{amsmath}

\title{STAT 222: Project 4\\
       Countable Care: Modeling Women's Health Care Decisions}
\author{Yuan He, Lindsey Lee, Jin Rou New, Tianyi Zhu\\
        University of California, Berkeley}
\date{\today}

\setlength\parindent{0pt} % to remove paragraph indent
\setlength{\parskip}{\baselineskip} % to get space between paragraphs
\usepackage{setspace} % to allow for double line spacing



\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}
\maketitle

\begin{abstract}
  A Bayesian analysis of a randomized response survey was carried out to study the use of cognition-enhancing drugs in the UCB student population and gender-specific differences in this use. About three quarters of the student population do not use cognition-enhancing drugs, while about a tenth use them with a prescription and 14\% use them without a prescription. We concluded that there is no statistically significant difference in drug use between male and female students. \\ 
	\hfill\break
	\textbf{Keywords}: countable care, Driven Data competition, women's health, machine learning, gradient boosting machine, random forest
\end{abstract}
\clearpage
%==================================================
\section{Introduction}
\label{sec:introduction}

The competition we have chosen to work on is DrivenData.org's Countable Care: Modeling Women's Health Care Decisions. The link to the competition can be found at: \href{http://www.drivendata.org/competitions/6/}{http://www.drivendata.org/competitions/6/}. DrivenData.org is the equivalent of Kaggle for social causes.
%==================================================
\section{Data}
\label{sec:data}

\subsection{Data description}
The data consists of responses by women in United States to the National Survey of Family Growth carried out by the United States Center for Disease Control and Prevention. Questions in the survey span topics such as demographics, marriage and reproductive health. There are a total of 14,644 observations in the training set and 3,661 observations in the test set, with each observation representing an individualâ€™s responses for one release of the survey (an individual may have participated in multiple rounds of the survey), and a total of 1378 features, including the survey round and 116 numeric (standardized), 211 ordinal and 1050 categorical features that are responses to survey questions. The data has been obfuscated, so it is not given what each feature corresponds to in real terms.

For each woman in the training set, a value of 1 or 0 was given for each of 14 health care services depending on whether she used that health care service in the last 12 months before the survey, with 1 corresponding to yes. Again, this data is also obfuscated so it is unknown what any of the health care services are. The distribution of the number of health care services used by each woman in the past year is shown in Fig~\ref{fig-num-svcs}. This is a long-tailed distribution with more than half the women using 3 or less services and very few using more than 5 services.

\begin{figure}[htbp]
    \begin{center}
  	\includegraphics[width = 0.4\textwidth]{fig/"number-of-svcs-used".pdf}
		\caption{\textbf{Distribution of number of health care services used by each woman.}}
		\label{fig-num-svcs}
  \end{center}
\end{figure}

The proportion of women using each health care service is given in Table~\ref{tab-prop-svc}. 



\begin{figure}[htbp]
    \begin{center}
    \includegraphics[width = 0.4\textwidth]{fig/"prop-women-using-each-service".pdf}
		\caption{\textbf{Proportion of women who used each health care service in the 12 months before the survey.}}
		\label{fig-num-svcs}
  \end{center}
\end{figure}

\begin{figure}[htbp]
  \begin{center}
  \begin{tabular}{|c|}
		\includegraphics[width = 0.3\textwidth]{fig/"prop-women-using-each-service".pdf} \\
		\includegraphics[width = 0.3\textwidth]{fig/"prop-women-using-each-service-survey-release-a".pdf}
    \includegraphics[width = 0.3\textwidth]{fig/"prop-women-using-each-service-survey-release-b".pdf}
    \includegraphics[width = 0.3\textwidth]{fig/"prop-women-using-each-service-survey-release-c".pdf}
    \end{tabular}
		\caption{\textbf{Proportion of women who used each health care service in the 12 months before the survey across all survey releases (top) and in each survey release (bottom).}}
		\label{fig-prop-svcs}
	\end{center}
\end{figure}

\begin{table}[ht]
  \centering
  \begin{tabular}{@{}|c|c|@{}}
  \hline
  Health care service & Proportion of women who used it \\ 
  \hline
  service\_a & 0.47 \\ 
  service\_b & 0.33 \\ 
  service\_c & 0.26 \\ 
  service\_d & 0.02 \\ 
  service\_e & 0.05 \\ 
  service\_f & 0.03 \\ 
  service\_g & 0.05 \\ 
  service\_h & 0.30 \\ 
  service\_i & 0.02 \\ 
  service\_j & 0.85 \\ 
  service\_k & 0.78 \\ 
  service\_l & 0.11 \\ 
  service\_m & 0.09 \\ 
  service\_n & 0.18 \\ 
  \hline
  \end{tabular}
	\caption{\textbf{Proportion of women using each of the 14 health care services.}} 
  \label{tab-prop-svc}
\end{table}

\subsection{Data problems}
The data is very sparse, with 82.6\% of the data missing in the train set. There are many features with a high proportion of missing values as shown in Fig~\ref{fig-missing}. This is because some survey questions depend on the response to previous survey questions and may be skipped.

\begin{figure}[htbp]
    \begin{center}
		\includegraphics[width = 0.4\textwidth]{fig/"prop-missing-in-columns".pdf}
		\caption{\textbf{Proportion of missing values in features.}}
		\label{fig-missing}
  \end{center}
\end{figure}

Secondly, columns of predictors/whether a women goes to a healthcare provider for different services may be dependent on each other. Fitting separate independent models for each health care service may result in a loss of information.

\subsection{Data processing}
\subsection{Feature engineering}
3 features were engineered and added to the data set. These are:
\begin{itemize}
  \item Number of numeric features with missing values for each woman
  \item Number of ordinal features with missing values for each woman
  \item Number of categorical features with missing values for each woman
\end{itemize}

The distribution of each feature across all women is given in Fig~\ref{fig-feature-engin}. Given the high proportion of missing values in the data set, the number of missing values for each women could be predictive. Moreover, these features capture some of the data that is lost in the next feature pruning step.

\begin{figure}[htbp]
    \begin{center}
  	\includegraphics[width = 1\textwidth]{fig/"dist-missing-values-across-women".pdf}
		\caption{\textbf{Distributions of missing values for different feature types.}}
		\label{fig-feature-engin}
  \end{center}
\end{figure}

\subsubsection{Feature pruning}
Given that there are 1378 features in the data set, it is unlikely that all of them would be strongly predictive of the dependent variables. In the first data processing step, features that would not be useful in modelling were dropped from both the training and test sets.

First of all, features that contain only missing values or one unique value (i.e. constant value for all women) were dropped from the data. There were 14 and 20 of such features respectively.

Secondly, features with a proportion of missing values exceeding a certain cut-off in the training set would be dropped, since missing value imputation is hardly meaningful for such features. If this cut-off was set to be 50\%, i.e. at least half of women in the training set answered a particular question, then 1159 out of 1378 features would be dropped. In comparison, with cut-offs of 80\%, 90\% and 95\%, 1038, 944 and 770 features would be dropped respectively. Different cut-offs were tested.

\subsubsection{Missing value imputation}
Missing values left were handled using different approaches for different feature types: numeric, ordinal or categorical. Since more than one data type was presented in the independent variables, plus that the dataset was huge and obfuscated, it was hard to use predictions from regression to fill in the NA values. Instead, generic methods were used for each datatype:
\begin{itemize}
  \item For numeric features, missing values were set to 0.
  \item For ordinal features, missing values were set to -1, as the lowest category for each ordinal feature is coded as either 0 or 1 in the training set.
  \item For categorical features, a new category ``missing'' was introduced and missing values were set to this category instead.
\end{itemize}

\subsubsection{Introducing Dummy Variables}
In order to fit certain algorithms (e.g. Ridge, SVM, logit), columns with data type "categorical" were required to be made into dummy variables. Function model.matrix in R was used to introduce dummy columns with categorical columns treated as factors.
Levels of each categorical variable were compared for train data and test data. Additional levels in test data that did not appear in train data were removed to make sure the models were trained properly. For levels that appeared in train data but not in test data, placeholder columns (columns with all 0's) were introduced to test data to take up the position of that level. This was necessary because it was required that train data and test data have the same number of columns, even after introducing dummy variables.

%==================================================
\section{Methods}
\label{sec:methods}

\subsection{Models}
We fitted a variety of models, including:
\begin{itemize}
  \item Gradient Boosting Machine (GBM)
\end{itemize}
\begin{itemize}
  \item Ridge Regression
\end{itemize}

\subsection{Computation}
All calculations were carried out in the \texttt{R} programming language, with additional functions from the \texttt{caret} package. Annotated code is given in the Appendix in Section \ref{sec:appendix}.
%==================================================
\section{Results} 
\label{sec:results}

\subsection{Prediction objective and evaluation metric}
Binary outcomes denoting if a woman went to a healthcare provider for each of 14 services in the 12 months preceding the survey are given in the training set. The prediction objective is to predict the probability for each of the 3,661 women in the test set of going to a healthcare provider for each of the 14 services. In other words, 3,661 x 14 predicted probabilities are required. The 14 services are not mutually exclusive; a woman can go to more than one service in the 12 months.

The evaluation metric here is the logarithmic loss, defined by $-\frac{1}{n}$. (FILL IN!)

\subsection{Prediction results}
A summary of the performance of our models on the test set is given in Table~\ref{tab-results}.

\begin{table}[ht]
  \centering
  \begin{tabular}{@{}|l|c|c|@{}}
    \hline
  	Data set & Model & Log loss\\ 
		\hline
  	Missing value cut-off of 50\% & Gradient boosting method & 0.2796 \\ 
    Missing value cut-off of 80\% & Gradient boosting method & 0.2812 \\
    Missing value cut-off of 50\% & Ridge Regression & 0.5044 \\
    Missing value cut-off of 80\% & Ridge Regression & 0.5274 \\
  	\hline
	\end{tabular}
	\caption{\textbf{Summary of model performance on test set.}}
  \label{tab-results}
\end{table}
%==================================================
\section{Discussion} 
\label{sec:discussion}

\subsection{Shortcomings and Future Developments}
One significant feature about this dataset was that it had not one, but fourteen dependent variables (Y's). And the dependent variables were correlated with each other since a woman usually orders more than one health services. Conventional regression models assumed the dimension of Y to be nx1, so did the algorithms in R. Therefore, 14 columns of data were fitted separately using the models, assuming they were independent, which was not actually true. Ignoring the correlation between Y variables may cause loss of accuracy. Application of multivariate methods might be able to increase prediction accuracy.

Methodology of imputing missing values might also result in loss of accuracy. Throwing away columns was a straightforward, but not elegant way to reduce dimension and save computer time. Ideally only columns with all NA's and columns with a single constant should be removed. Moreover, replacing missing values with a global value (e.g. 0 or -1) was a compromise due to the large amount of NA's and obfuscation of dataset. Provided more valid data and smaller size, regressing the missing values on other columns should be a better solution.

Procedure of creating dummy variables could be another source of problem. First of all, some levels in test data had to be dropped (these levels were replaced by "missing"). Secondly, introducing dummy variables dramatically increased dimension of data, without bringing in extra information. Lastly, the sole purpose of introducing placeholder columns (columns with only 0's) in test data was to make the number of columns match for test and train. Given that the model permitted, using factors instead of dummy variables should be more concise.

When regression models yielded prediction for probabilities, some of them might fall out of the range of 0 to 1. Converting these over-bound values to 0 or 1 would dramatically increase the score since log-loss was used for evaluation of prediction. To avoid this problem, values over-bound were replaced with column means. However, a finer way should be adopted to accommodate those values.  

%==================================================
\section{References}
\clearpage
%==================================================
\appendix
\section*{Appendix}
\label{sec:appendix}


\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{rm}\hlstd{(}\hlkwc{list} \hlstd{=} \hlkwd{ls}\hlstd{())}
\hlkwd{gc}\hlstd{()}
\hlstd{run_on_server} \hlkwb{<-} \hlnum{FALSE} \hlcom{###}
\hlkwa{if} \hlstd{(}\hlopt{!}\hlstd{run_on_server)}
  \hlkwd{setwd}\hlstd{(}\hlstr{"~/Copy/Berkeley/stat222-spring-2015/stat222sp15/projects/countable-care"}\hlstd{)}
\hlstd{data_dir} \hlkwb{<-} \hlstr{"data"}
\hlstd{fig_dir} \hlkwb{<-} \hlstr{"fig"}
\hlstd{results_dir} \hlkwb{<-} \hlstr{"results"}
\hlkwd{dir.create}\hlstd{(fig_dir,} \hlkwc{showWarnings} \hlstd{=} \hlnum{FALSE}\hlstd{)}
\hlkwd{dir.create}\hlstd{(results_dir,} \hlkwc{showWarnings} \hlstd{=} \hlnum{FALSE}\hlstd{)}
\hlkwd{dir.create}\hlstd{(}\hlstr{"submit"}\hlstd{,} \hlkwc{showWarnings} \hlstd{=} \hlnum{FALSE}\hlstd{)}
\hlstd{get_notifications} \hlkwb{<-} \hlkwd{ifelse}\hlstd{(run_on_server,} \hlnum{TRUE}\hlstd{,} \hlnum{FALSE}\hlstd{)}
\hlkwa{if} \hlstd{(get_notifications) \{}
  \hlkwd{library}\hlstd{(RPushbullet)}
  \hlcom{# options(error = function() \{ # Be notified when there is an error}
  \hlcom{#   pbPost("note", "Error!", geterrmessage(), recipients = c(1, 2))}
  \hlcom{# \})}
\hlstd{\}}

\hlstd{write_submission} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{probs}\hlstd{,} \hlkwc{model_name}\hlstd{) \{}
  \hlstd{file_path} \hlkwb{<-} \hlkwd{file.path}\hlstd{(}\hlstr{"submit"}\hlstd{,} \hlkwd{paste0}\hlstd{(model_name,} \hlstr{".csv"}\hlstd{))}
  \hlstd{submit} \hlkwb{<-} \hlkwd{read.csv}\hlstd{(}\hlstr{"data/SubmissionFormat.csv"}\hlstd{)}
  \hlstd{submit[,} \hlnum{2}\hlopt{:}\hlkwd{ncol}\hlstd{(submit)]} \hlkwb{<-} \hlstd{probs}
  \hlkwa{if} \hlstd{(}\hlkwd{file.exists}\hlstd{(file_path))}
    \hlkwd{stop}\hlstd{(}\hlkwd{paste0}\hlstd{(file_path,} \hlstr{" already exists!"}\hlstd{))}
  \hlkwd{write.csv}\hlstd{(submit,} \hlkwd{file.path}\hlstd{(file_path),} \hlkwc{row.names} \hlstd{=} \hlnum{FALSE}\hlstd{)}
  \hlkwd{message}\hlstd{(}\hlkwd{paste0}\hlstd{(}\hlstr{"Results written to "}\hlstd{, file_path))}
\hlstd{\}}

\hlstd{seed} \hlkwb{<-} \hlnum{12345}
\hlkwd{set.seed}\hlstd{(seed)}
\hlkwd{library}\hlstd{(caret)}
\hlkwd{library}\hlstd{(e1071)}
\hlcom{# List all models in caret}
\hlcom{# names(getModelInfo())}

\hlcom{# Load data}
\hlstd{prop_missing_cutoff} \hlkwb{<-} \hlnum{0.5}
\hlkwd{load}\hlstd{(}\hlkwc{file} \hlstd{=} \hlkwd{file.path}\hlstd{(data_dir,} \hlkwd{paste0}\hlstd{(}\hlstr{"data_cutoff"}\hlstd{, prop_missing_cutoff,} \hlstr{".rda"}\hlstd{)))}
\hlstd{train} \hlkwb{<-} \hlstd{data}\hlopt{$}\hlstd{train}
\hlstd{test} \hlkwb{<-} \hlstd{data}\hlopt{$}\hlstd{test}
\hlstd{ytrain} \hlkwb{<-} \hlstd{data}\hlopt{$}\hlstd{ytrain}

\hlcom{# Create data partitions of 80% and 20%}
\hlstd{ntrain} \hlkwb{<-} \hlkwd{nrow}\hlstd{(train)}
\hlstd{train_indices} \hlkwb{<-} \hlkwd{sample}\hlstd{(}\hlnum{1}\hlopt{:}\hlstd{ntrain)[}\hlnum{1}\hlopt{:}\hlkwd{floor}\hlstd{(ntrain}\hlopt{*}\hlnum{0.8}\hlstd{)]}
\hlstd{train_val} \hlkwb{<-} \hlstd{train[}\hlopt{-}\hlstd{train_indices, ]}

\hlcom{# Set up caret models}
\hlstd{train_control} \hlkwb{<-} \hlkwd{trainControl}\hlstd{(}\hlkwc{method} \hlstd{=} \hlstr{"cv"}\hlstd{,} \hlkwc{number} \hlstd{=} \hlnum{10}\hlstd{,} \hlkwc{returnResamp} \hlstd{=} \hlstr{"none"}\hlstd{)}

\hlcom{# mod_types <- c("gbm", "rf")}
\hlstd{mod_types} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlstr{"rf"}\hlstd{)}
\hlstd{mod} \hlkwb{<-} \hlkwd{list}\hlstd{()}
\hlstd{probs} \hlkwb{<-} \hlkwd{matrix}\hlstd{(}\hlnum{NA}\hlstd{,} \hlkwd{nrow}\hlstd{(test),} \hlkwd{ncol}\hlstd{(ytrain))}
\hlkwa{for} \hlstd{(mod_type} \hlkwa{in} \hlstd{mod_types) \{}
  \hlkwa{for} \hlstd{(svc_index} \hlkwa{in} \hlnum{1}\hlopt{:}\hlkwd{ncol}\hlstd{(ytrain)) \{}

    \hlcom{# Testing!!!}
    \hlcom{# mod_type <- "rf"}
    \hlcom{# train_indices <- 1:100}
    \hlcom{# svc_index <- 1}

    \hlcom{# Train all the models with train data}
    \hlstd{mod[[svc_index]]} \hlkwb{<-} \hlkwd{train}\hlstd{(train[train_indices, ], ytrain[train_indices, svc_index],}
                              \hlkwc{method} \hlstd{= mod_type,} \hlkwc{trControl} \hlstd{= train_control)}

    \hlcom{# Predict on test data}
    \hlstd{probs[, svc_index]} \hlkwb{<-} \hlkwd{predict}\hlstd{(}\hlkwc{object} \hlstd{= mod[[svc_index]],} \hlkwc{newdata} \hlstd{= test,}
                                  \hlkwc{type} \hlstd{=} \hlstr{"prob"}\hlstd{)}\hlopt{$}\hlstd{yes}

    \hlcom{# Get predictions for each model and add them back to themselves}
    \hlcom{# train_val[[paste0(mod_type, "_PROB"]] <- predict(mod[[svc_index]], train_val, type = "prob")}
    \hlcom{# test[[paste0(mod_type, "_PROB"]] <- predict(mod[[svc_index]], test, type = "prob")}

    \hlcom{# Run an ensemble model to blend all the predicted probabilities}
    \hlcom{# mod_ensemble[[svc_index]] <- train(train_val, ytrain[-train_indices, svc_index], }
    \hlcom{#                                    method = "lasso", trControl = train_control)}

    \hlcom{# Predict on test data}
    \hlcom{# preds <- predict(mod_ensemble[[svc_index]], test, type = "prob")}
  \hlstd{\}}
  \hlkwd{write_submission}\hlstd{(probs,} \hlkwd{paste0}\hlstd{(mod_type,} \hlstr{"_cutoff"}\hlstd{, prop_missing_cutoff))}
  \hlkwd{save}\hlstd{(mod,} \hlkwc{file} \hlstd{=} \hlkwd{file.path}\hlstd{(results_dir,}
                             \hlkwd{paste0}\hlstd{(}\hlstr{"mod_"}\hlstd{, mod_type,} \hlstr{"_cutoff"}\hlstd{, prop_missing_cutoff,} \hlstr{".rda"}\hlstd{)))}
  \hlkwa{if} \hlstd{(get_notifications)}
    \hlkwd{pbPost}\hlstd{(}\hlkwc{type} \hlstd{=} \hlstr{"note"}\hlstd{,}
           \hlkwc{title} \hlstd{=} \hlstr{"stat222"}\hlstd{,}
           \hlkwc{body} \hlstd{=} \hlkwd{paste0}\hlstd{(mod_type,} \hlstr{" done!"}\hlstd{),}
           \hlkwc{recipients} \hlstd{=} \hlkwd{c}\hlstd{(}\hlnum{1}\hlstd{,} \hlnum{2}\hlstd{))}
\hlstd{\}}

\hlcom{# # Random probability drawn from U(0, 1)}
\hlcom{# probs <- matrix(runif(nrow(test)*(ncol(ytrain))), nrow(test), ncol(ytrain))}
\hlcom{# write_submission(probs, paste0("unifseed", seed))}
\hlcom{# # Constant probability of 0.5}
\hlcom{# probs <- matrix(0.5, nrow(test), ncol(ytrain))}
\hlcom{# write_submission(probs, "constant0.5")}
\hlcom{# # Constant probability of proportion for each service}
\hlcom{# ytrain_props <- sapply(ytrain, mean)}
\hlcom{# probs <- matrix(rep(ytrain_props, each = nrow(test)), nrow(test), ncol(ytrain))}
\hlcom{# write_submission(probs, "constantprop")}
\end{alltt}
\end{kframe}
\end{knitrout}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{rm}\hlstd{(}\hlkwc{list} \hlstd{=} \hlkwd{ls}\hlstd{())}
\hlkwd{gc}\hlstd{()}
\hlkwd{setwd}\hlstd{(}\hlstr{"~/Copy/Berkeley/stat222-spring-2015/stat222sp15/projects/countable-care"}\hlstd{)}
\hlstd{data_dir} \hlkwb{<-} \hlstr{"data"}
\hlstd{fig_dir} \hlkwb{<-} \hlstr{"fig"}
\hlstd{results_dir} \hlkwb{<-} \hlstr{"results"}
\hlkwd{dir.create}\hlstd{(fig_dir,} \hlkwc{showWarnings} \hlstd{=} \hlnum{FALSE}\hlstd{)}
\hlkwd{dir.create}\hlstd{(results_dir,} \hlkwc{showWarnings} \hlstd{=} \hlnum{FALSE}\hlstd{)}
\hlkwd{dir.create}\hlstd{(}\hlstr{"submit"}\hlstd{,} \hlkwc{showWarnings} \hlstd{=} \hlnum{FALSE}\hlstd{)}
\hlstd{prop_missing_cutoff} \hlkwb{<-} \hlnum{0.95}

\hlcom{# Read in data}
\hlstd{train_readin} \hlkwb{<-} \hlkwd{read.csv}\hlstd{(}\hlkwd{file.path}\hlstd{(data_dir,} \hlstr{"train_values.csv"}\hlstd{),}
                         \hlkwc{stringsAsFactors} \hlstd{=} \hlnum{FALSE}\hlstd{,} \hlkwc{na.strings} \hlstd{=} \hlstr{""}\hlstd{)}
\hlstd{ytrain} \hlkwb{<-} \hlkwd{read.csv}\hlstd{(}\hlkwd{file.path}\hlstd{(data_dir,} \hlstr{"train_labels.csv"}\hlstd{))}
\hlstd{test_readin} \hlkwb{<-} \hlkwd{read.csv}\hlstd{(}\hlkwd{file.path}\hlstd{(data_dir,} \hlstr{"test_values.csv"}\hlstd{),}
                        \hlkwc{stringsAsFactors} \hlstd{=} \hlnum{FALSE}\hlstd{,} \hlkwc{na.strings} \hlstd{=} \hlstr{""}\hlstd{)}

\hlcom{# Data exploration}
\hlcom{# Column types}
\hlstd{colnames_all} \hlkwb{<-} \hlkwd{names}\hlstd{(train_readin)}
\hlstd{colnames_type} \hlkwb{<-} \hlkwd{sapply}\hlstd{(colnames_all,} \hlkwa{function}\hlstd{(}\hlkwc{x}\hlstd{)} \hlkwd{strsplit}\hlstd{(x,} \hlstr{"_"}\hlstd{)[[}\hlnum{1}\hlstd{]][}\hlnum{1}\hlstd{])}
\hlkwd{table}\hlstd{(colnames_type[}\hlopt{-}\hlkwd{c}\hlstd{(}\hlnum{1}\hlstd{,} \hlnum{2}\hlstd{)])}

\hlcom{# Check proportion of missing values in features}
\hlstd{prop_missing} \hlkwb{<-} \hlkwd{sapply}\hlstd{(train_readin,} \hlkwa{function}\hlstd{(}\hlkwc{x}\hlstd{)} \hlkwd{mean}\hlstd{(}\hlkwd{is.na}\hlstd{(x)))}
\hlkwd{par}\hlstd{(}\hlkwc{mar} \hlstd{=} \hlkwd{c}\hlstd{(}\hlnum{4.5}\hlstd{,} \hlnum{4.5}\hlstd{,} \hlnum{4.5}\hlstd{,} \hlnum{2}\hlstd{))}
\hlkwd{pdf}\hlstd{(}\hlkwd{file.path}\hlstd{(fig_dir,} \hlstr{"prop-missing-in-columns.pdf"}\hlstd{),} \hlkwc{width} \hlstd{=} \hlnum{7}\hlstd{,} \hlkwc{height} \hlstd{=} \hlnum{5}\hlstd{)}
\hlkwd{hist}\hlstd{(prop_missing,} \hlkwc{main} \hlstd{=} \hlstr{"Proportion of missing values\textbackslash{}nin features"}\hlstd{,}
     \hlkwc{xlab} \hlstd{=} \hlstr{"Proportion"}\hlstd{,} \hlkwc{ylab} \hlstd{=} \hlstr{"Number of features"}\hlstd{,} \hlkwc{col} \hlstd{=} \hlstr{"lightgrey"}\hlstd{)}
\hlkwd{dev.off}\hlstd{()}

\hlcom{# Data processing}
\hlcom{# Check original number of features, not including id}
\hlkwd{ncol}\hlstd{(train_readin[,} \hlopt{-}\hlnum{1}\hlstd{])} \hlcom{# 1378}

\hlcom{# Check for features with only constant values}
\hlstd{cols_constant} \hlkwb{<-} \hlkwd{sapply}\hlstd{(train_readin,} \hlkwa{function}\hlstd{(}\hlkwc{x}\hlstd{)} \hlkwd{length}\hlstd{(}\hlkwd{unique}\hlstd{(x))} \hlopt{==} \hlnum{1}\hlstd{)}
\hlkwd{sum}\hlstd{(cols_constant)} \hlcom{# 20}

\hlcom{# Feature engineering}
\hlcom{# Column types}
\hlstd{colnames_all} \hlkwb{<-} \hlkwd{names}\hlstd{(train_readin)}
\hlstd{colnames_type} \hlkwb{<-} \hlkwd{sapply}\hlstd{(colnames_all,} \hlkwa{function}\hlstd{(}\hlkwc{x}\hlstd{)} \hlkwd{strsplit}\hlstd{(x,} \hlstr{"_"}\hlstd{)[[}\hlnum{1}\hlstd{]][}\hlnum{1}\hlstd{])}
\hlkwd{table}\hlstd{(colnames_type[}\hlopt{-}\hlkwd{c}\hlstd{(}\hlnum{1}\hlstd{,} \hlnum{2}\hlstd{)])}
\hlstd{cols_numeric} \hlkwb{<-} \hlstd{colnames_type} \hlopt{==} \hlstr{"n"}
\hlstd{cols_ordinal} \hlkwb{<-} \hlstd{colnames_type} \hlopt{==} \hlstr{"o"}
\hlstd{cols_categorical} \hlkwb{<-} \hlstd{colnames_type} \hlopt{==} \hlstr{"c"}

\hlcom{# Number of missing numeric/ordinal/categorical features}
\hlstd{num_missing_numeric} \hlkwb{<-} \hlkwd{apply}\hlstd{(train_readin[, cols_numeric],} \hlnum{1}\hlstd{,} \hlkwa{function}\hlstd{(}\hlkwc{x}\hlstd{)} \hlkwd{sum}\hlstd{(}\hlkwd{is.na}\hlstd{(x)))}
\hlstd{num_missing_ordinal} \hlkwb{<-} \hlkwd{apply}\hlstd{(train_readin[, cols_ordinal],} \hlnum{1}\hlstd{,} \hlkwa{function}\hlstd{(}\hlkwc{x}\hlstd{)} \hlkwd{sum}\hlstd{(}\hlkwd{is.na}\hlstd{(x)))}
\hlstd{num_missing_categorical} \hlkwb{<-} \hlkwd{apply}\hlstd{(train_readin[, cols_categorical],} \hlnum{1}\hlstd{,} \hlkwa{function}\hlstd{(}\hlkwc{x}\hlstd{)} \hlkwd{sum}\hlstd{(}\hlkwd{is.na}\hlstd{(x)))}
\hlkwd{pdf}\hlstd{(}\hlkwd{file.path}\hlstd{(fig_dir,} \hlstr{"dist-missing-values-across-women.pdf"}\hlstd{),} \hlkwc{width} \hlstd{=} \hlnum{12}\hlopt{/}\hlnum{1.2}\hlstd{,} \hlnum{3.5}\hlopt{/}\hlnum{1.2}\hlstd{)}
\hlkwd{par}\hlstd{(}\hlkwc{mar} \hlstd{=} \hlkwd{c}\hlstd{(}\hlnum{4.5}\hlstd{,} \hlnum{4.5}\hlstd{,} \hlnum{1}\hlstd{,} \hlnum{1}\hlstd{),} \hlkwc{mfrow} \hlstd{=} \hlkwd{c}\hlstd{(}\hlnum{1}\hlstd{,} \hlnum{3}\hlstd{))}
\hlkwd{hist}\hlstd{(num_missing_numeric,} \hlkwc{freq} \hlstd{=} \hlnum{FALSE}\hlstd{,} \hlkwc{breaks} \hlstd{=} \hlnum{50}\hlstd{,}
     \hlkwc{main} \hlstd{=} \hlstr{""}\hlstd{,} \hlkwc{col} \hlstd{=} \hlstr{"lightgrey"}\hlstd{,}
     \hlkwc{xlab} \hlstd{=} \hlstr{"Number of missing values for numeric features"}\hlstd{)}
\hlkwd{legend}\hlstd{(}\hlstr{"topleft"}\hlstd{,} \hlkwc{legend} \hlstd{=} \hlkwd{c}\hlstd{(}\hlkwd{paste0}\hlstd{(}\hlstr{"Mean = "}\hlstd{,} \hlkwd{round}\hlstd{(}\hlkwd{mean}\hlstd{(num_missing_numeric),} \hlkwc{digits} \hlstd{=} \hlnum{1}\hlstd{),}
                                    \hlstr{"\textbackslash{}nMedian = "}\hlstd{,} \hlkwd{median}\hlstd{(num_missing_numeric),}
                                    \hlstr{"\textbackslash{}nSD = "}\hlstd{,} \hlkwd{round}\hlstd{(}\hlkwd{sd}\hlstd{(num_missing_numeric),} \hlkwc{digits} \hlstd{=} \hlnum{1}\hlstd{))),}
       \hlkwc{bty} \hlstd{=} \hlstr{"n"}\hlstd{)}
\hlkwd{hist}\hlstd{(num_missing_ordinal,} \hlkwc{freq} \hlstd{=} \hlnum{FALSE}\hlstd{,} \hlkwc{breaks} \hlstd{=} \hlnum{50}\hlstd{,}
     \hlkwc{main} \hlstd{=} \hlstr{""}\hlstd{,} \hlkwc{col} \hlstd{=} \hlstr{"lightgrey"}\hlstd{,}
     \hlkwc{xlab} \hlstd{=} \hlstr{"Number of missing values for ordinal features"}\hlstd{)}
\hlkwd{legend}\hlstd{(}\hlstr{"topleft"}\hlstd{,} \hlkwc{legend} \hlstd{=} \hlkwd{c}\hlstd{(}\hlkwd{paste0}\hlstd{(}\hlstr{"Mean = "}\hlstd{,} \hlkwd{round}\hlstd{(}\hlkwd{mean}\hlstd{(num_missing_ordinal),} \hlkwc{digits} \hlstd{=} \hlnum{1}\hlstd{),}
                                    \hlstr{"\textbackslash{}nMedian = "}\hlstd{,} \hlkwd{median}\hlstd{(num_missing_ordinal),}
                                    \hlstr{"\textbackslash{}nSD = "}\hlstd{,} \hlkwd{round}\hlstd{(}\hlkwd{sd}\hlstd{(num_missing_ordinal),} \hlkwc{digits} \hlstd{=} \hlnum{1}\hlstd{))),}
       \hlkwc{bty} \hlstd{=} \hlstr{"n"}\hlstd{)}
\hlkwd{hist}\hlstd{(num_missing_categorical,} \hlkwc{freq} \hlstd{=} \hlnum{FALSE}\hlstd{,} \hlkwc{breaks} \hlstd{=} \hlnum{50}\hlstd{,}
     \hlkwc{main} \hlstd{=} \hlstr{""}\hlstd{,} \hlkwc{col} \hlstd{=} \hlstr{"lightgrey"}\hlstd{,}
     \hlkwc{xlab} \hlstd{=} \hlstr{"Number of missing values for categorical features"}\hlstd{)}
\hlkwd{legend}\hlstd{(}\hlstr{"topleft"}\hlstd{,} \hlkwc{legend} \hlstd{=} \hlkwd{c}\hlstd{(}\hlkwd{paste0}\hlstd{(}\hlstr{"Mean = "}\hlstd{,} \hlkwd{round}\hlstd{(}\hlkwd{mean}\hlstd{(num_missing_categorical),} \hlkwc{digits} \hlstd{=} \hlnum{1}\hlstd{),}
                                    \hlstr{"\textbackslash{}nMedian = "}\hlstd{,} \hlkwd{median}\hlstd{(num_missing_categorical),}
                                    \hlstr{"\textbackslash{}nSD = "}\hlstd{,} \hlkwd{round}\hlstd{(}\hlkwd{sd}\hlstd{(num_missing_categorical),} \hlkwc{digits} \hlstd{=} \hlnum{1}\hlstd{))),}
       \hlkwc{bty} \hlstd{=} \hlstr{"n"}\hlstd{)}
\hlkwd{dev.off}\hlstd{()}
\hlstd{num_missing_numeric_test} \hlkwb{<-} \hlkwd{apply}\hlstd{(test_readin[, cols_numeric],} \hlnum{1}\hlstd{,} \hlkwa{function}\hlstd{(}\hlkwc{x}\hlstd{)} \hlkwd{sum}\hlstd{(}\hlkwd{is.na}\hlstd{(x)))}
\hlstd{num_missing_ordinal_test} \hlkwb{<-} \hlkwd{apply}\hlstd{(test_readin[, cols_ordinal],} \hlnum{1}\hlstd{,} \hlkwa{function}\hlstd{(}\hlkwc{x}\hlstd{)} \hlkwd{sum}\hlstd{(}\hlkwd{is.na}\hlstd{(x)))}
\hlstd{num_missing_categorical_test} \hlkwb{<-} \hlkwd{apply}\hlstd{(test_readin[, cols_categorical],} \hlnum{1}\hlstd{,} \hlkwa{function}\hlstd{(}\hlkwc{x}\hlstd{)} \hlkwd{sum}\hlstd{(}\hlkwd{is.na}\hlstd{(x)))}

\hlcom{# Check for features with proportion of missing values > prop_missing_cutoff}
\hlstd{prop_missing} \hlkwb{<-} \hlkwd{sapply}\hlstd{(train_readin,} \hlkwa{function}\hlstd{(}\hlkwc{x}\hlstd{)} \hlkwd{mean}\hlstd{(}\hlkwd{is.na}\hlstd{(x)))}
\hlstd{cols_missing} \hlkwb{<-} \hlstd{prop_missing} \hlopt{>} \hlstd{prop_missing_cutoff}
\hlkwd{sum}\hlstd{(cols_missing)}

\hlcom{# Remaining number of features, not including id}
\hlkwd{sum}\hlstd{(}\hlopt{!}\hlstd{cols_constant} \hlopt{& !}\hlstd{cols_missing)} \hlopt{-} \hlnum{1}

\hlcom{# Remove above features and id}
\hlstd{train} \hlkwb{<-} \hlstd{train_readin[,} \hlkwd{names}\hlstd{(train_readin)} \hlopt{!=} \hlstr{"id"} \hlopt{& !}\hlstd{cols_constant} \hlopt{& !}\hlstd{cols_missing]}
\hlstd{test} \hlkwb{<-} \hlstd{test_readin[,} \hlkwd{names}\hlstd{(train_readin)} \hlopt{!=} \hlstr{"id"} \hlopt{& !}\hlstd{cols_constant} \hlopt{& !}\hlstd{cols_missing]}

\hlcom{# Column types}
\hlstd{colnames_all} \hlkwb{<-} \hlkwd{names}\hlstd{(train)}
\hlstd{colnames_type} \hlkwb{<-} \hlkwd{sapply}\hlstd{(colnames_all,} \hlkwa{function}\hlstd{(}\hlkwc{x}\hlstd{)} \hlkwd{strsplit}\hlstd{(x,} \hlstr{"_"}\hlstd{)[[}\hlnum{1}\hlstd{]][}\hlnum{1}\hlstd{])}
\hlkwd{table}\hlstd{(colnames_type[}\hlopt{-}\hlkwd{c}\hlstd{(}\hlnum{1}\hlstd{,} \hlnum{2}\hlstd{)])}
\hlstd{cols_numeric} \hlkwb{<-} \hlstd{colnames_type} \hlopt{==} \hlstr{"n"}
\hlstd{cols_ordinal} \hlkwb{<-} \hlstd{colnames_type} \hlopt{==} \hlstr{"o"}
\hlstd{cols_categorical} \hlkwb{<-} \hlstd{colnames_type} \hlopt{==} \hlstr{"c"}

\hlcom{# Missing value imputation for remaining features}
\hlcom{# Numeric features: Set as 0}
\hlkwa{for} \hlstd{(i} \hlkwa{in} \hlnum{1}\hlopt{:}\hlkwd{sum}\hlstd{(cols_numeric)) \{}
  \hlstd{train[, cols_numeric][, i]} \hlkwb{<-} \hlkwd{ifelse}\hlstd{(}\hlkwd{is.na}\hlstd{(train[, cols_numeric][, i]),}
                                       \hlnum{0}\hlstd{, train[, cols_numeric][, i])}
  \hlstd{test[, cols_numeric][, i]} \hlkwb{<-} \hlkwd{ifelse}\hlstd{(}\hlkwd{is.na}\hlstd{(test[, cols_numeric][, i]),}
                                      \hlnum{0}\hlstd{, test[, cols_numeric][, i])}
\hlstd{\}}
\hlcom{# Ordinal features: Set as -1}
\hlcom{# table(sapply(train[, cols_ordinal], min, na.rm = TRUE)) # min is 0 or 1}
\hlkwa{for} \hlstd{(i} \hlkwa{in} \hlnum{1}\hlopt{:}\hlkwd{sum}\hlstd{(cols_ordinal)) \{}
  \hlstd{train[, cols_ordinal][, i]} \hlkwb{<-} \hlkwd{as.integer}\hlstd{(}\hlkwd{ifelse}\hlstd{(}\hlkwd{is.na}\hlstd{(train[, cols_ordinal][, i]),}
                                              \hlopt{-}\hlnum{1}\hlstd{, train[, cols_ordinal][, i]))}
  \hlstd{test[, cols_ordinal][, i]} \hlkwb{<-} \hlkwd{as.integer}\hlstd{(}\hlkwd{ifelse}\hlstd{(}\hlkwd{is.na}\hlstd{(test[, cols_ordinal][, i]),}
                                             \hlopt{-}\hlnum{1}\hlstd{, test[, cols_ordinal][, i]))}
\hlstd{\}}

\hlcom{# Categorical features}
\hlcom{# Check for: i) features with categories in test but not train set.}
\hlcom{# ii) features with missing values in test but not train set}
\hlstd{cols_unknownlevels} \hlkwb{<-} \hlkwa{NULL}
\hlstd{cols_nomissingintrain} \hlkwb{<-} \hlkwa{NULL}
\hlkwa{for} \hlstd{(i} \hlkwa{in} \hlnum{1}\hlopt{:}\hlkwd{sum}\hlstd{(cols_categorical)) \{}
  \hlkwa{if} \hlstd{(}\hlkwd{any}\hlstd{(}\hlkwd{is.na}\hlstd{(test[, cols_categorical][, i]))} \hlopt{&} \hlkwd{all}\hlstd{(}\hlopt{!}\hlkwd{is.na}\hlstd{(train[, cols_categorical][, i]))) \{}
    \hlkwd{print}\hlstd{(i)}
    \hlstd{cols_nomissingintrain} \hlkwb{<-} \hlkwd{c}\hlstd{(cols_nomissingintrain, i)}
  \hlstd{\}}
  \hlkwa{if} \hlstd{(}\hlkwd{any}\hlstd{(}\hlopt{!}\hlkwd{is.na}\hlstd{(test[, cols_categorical][, i])} \hlopt{&}
            \hlopt{!}\hlstd{(test[, cols_categorical][, i]} \hlopt{%in%} \hlkwd{unique}\hlstd{(train[, cols_categorical][, i])))) \{}
    \hlstd{cols_unknownlevels} \hlkwb{<-} \hlkwd{c}\hlstd{(cols_unknownlevels, i)}
    \hlstd{levels_train} \hlkwb{<-} \hlkwd{unique}\hlstd{(train[, cols_categorical][, i])}
    \hlstd{levels_train} \hlkwb{<-} \hlkwd{ifelse}\hlstd{(}\hlkwd{is.na}\hlstd{(levels_train),} \hlstr{"missing"}\hlstd{,} \hlkwd{as.character}\hlstd{(levels_train))}
    \hlstd{levels_test} \hlkwb{<-} \hlkwd{unique}\hlstd{(test[, cols_categorical][, i])}
    \hlstd{levels_test} \hlkwb{<-} \hlkwd{ifelse}\hlstd{(}\hlkwd{is.na}\hlstd{(levels_test),} \hlstr{"missing"}\hlstd{,} \hlkwd{as.character}\hlstd{(levels_test))}
    \hlkwa{if} \hlstd{(}\hlopt{!}\hlstd{(}\hlstr{"missing"} \hlopt{%in%} \hlstd{levels_train)) \{}
      \hlkwd{print}\hlstd{(i)}
      \hlkwd{print}\hlstd{(levels_test[}\hlopt{!}\hlstd{(levels_test} \hlopt{%in%} \hlstd{levels_train)])}
      \hlkwd{print}\hlstd{(}\hlstr{"---"}\hlstd{)}
      \hlstd{cols_nomissingintrain} \hlkwb{<-} \hlkwd{c}\hlstd{(cols_nomissingintrain,}
                                 \hlkwd{rep}\hlstd{(i,} \hlkwd{length}\hlstd{(levels_test[}\hlopt{!}\hlstd{(levels_test} \hlopt{%in%} \hlstd{levels_train)])))}
    \hlstd{\}}
  \hlstd{\}}
\hlstd{\}}

\hlcom{# Categorical features: Set as new category missing}
\hlkwa{for} \hlstd{(i} \hlkwa{in} \hlnum{1}\hlopt{:}\hlkwd{sum}\hlstd{(cols_categorical)) \{}
  \hlstd{train[, cols_categorical][, i]} \hlkwb{<-} \hlkwd{as.factor}\hlstd{(}\hlkwd{ifelse}\hlstd{(}\hlkwd{is.na}\hlstd{(train[, cols_categorical][, i]),}
                                                  \hlstr{"missing"}\hlstd{, train[, cols_categorical][, i]))}
  \hlstd{test[, cols_categorical][, i]} \hlkwb{<-} \hlkwd{as.factor}\hlstd{(}\hlkwd{ifelse}\hlstd{(}\hlkwd{is.na}\hlstd{(test[, cols_categorical][, i]),}
                                                    \hlstr{"missing"}\hlstd{,}
                                                    \hlkwd{ifelse}\hlstd{(}\hlopt{!}\hlstd{(test[, cols_categorical][, i]} \hlopt{%in%}
                                                      \hlkwd{unique}\hlstd{(train[, cols_categorical][, i])),}
                                                      \hlstr{"missing"}\hlstd{, test[, cols_categorical][, i])))}
\hlstd{\}}

\hlcom{# Set values in test set to most frequently-occurring category in train set for:}
\hlcom{# i) features with categories in test but not train set.}
\hlcom{# ii) features with missing values in test but not train set}
\hlkwa{for} \hlstd{(c} \hlkwa{in} \hlkwd{seq_along}\hlstd{(cols_nomissingintrain)) \{}
  \hlstd{i} \hlkwb{<-} \hlstd{cols_nomissingintrain[c]}
  \hlstd{value_new} \hlkwb{<-} \hlkwd{names}\hlstd{(}\hlkwd{which.max}\hlstd{(}\hlkwd{table}\hlstd{(test[, cols_categorical][, i])))}
  \hlstd{test[, cols_categorical][, i]} \hlkwb{<-} \hlkwd{as.factor}\hlstd{(}\hlkwd{ifelse}\hlstd{(}\hlkwd{as.character}\hlstd{(test[, cols_categorical][, i])} \hlopt{==}
                                                      \hlstr{"missing"}\hlstd{,}
                                                    \hlstd{value_new,}
                                                    \hlkwd{as.character}\hlstd{(test[, cols_categorical][, i])))}
\hlstd{\}}

\hlcom{# For random forest, ensure that categorical features have same}
\hlcom{# levels in train and test sets}
\hlkwa{for} \hlstd{(i} \hlkwa{in} \hlnum{1}\hlopt{:}\hlkwd{sum}\hlstd{(cols_categorical)) \{}
  \hlstd{test[, cols_categorical][, i]} \hlkwb{<-} \hlkwd{factor}\hlstd{(test[, cols_categorical][, i],}
                                          \hlkwc{levels} \hlstd{=} \hlkwd{levels}\hlstd{(train[, cols_categorical][, i]))}

\hlstd{\}}

\hlcom{# Add engineered features to data}
\hlstd{train}\hlopt{$}\hlstd{num_missing_numeric} \hlkwb{<-} \hlstd{num_missing_numeric}
\hlstd{train}\hlopt{$}\hlstd{num_missing_ordinal} \hlkwb{<-} \hlstd{num_missing_ordinal}
\hlstd{train}\hlopt{$}\hlstd{num_missing_categorical} \hlkwb{<-} \hlstd{num_missing_categorical}
\hlstd{test}\hlopt{$}\hlstd{num_missing_numeric} \hlkwb{<-} \hlstd{num_missing_numeric_test}
\hlstd{test}\hlopt{$}\hlstd{num_missing_ordinal} \hlkwb{<-} \hlstd{num_missing_ordinal_test}
\hlstd{test}\hlopt{$}\hlstd{num_missing_categorical} \hlkwb{<-} \hlstd{num_missing_categorical_test}

\hlcom{# Convert release variable to factor, else it throws error}
\hlstd{train}\hlopt{$}\hlstd{release} \hlkwb{<-} \hlkwd{as.factor}\hlstd{(train}\hlopt{$}\hlstd{release)}
\hlstd{test}\hlopt{$}\hlstd{release} \hlkwb{<-} \hlkwd{as.factor}\hlstd{(test}\hlopt{$}\hlstd{release)}

\hlcom{# Convert prediction label to alphabetical factor, }
\hlcom{# else it throws error in caret::predict}
\hlkwa{for} \hlstd{(i} \hlkwa{in} \hlnum{1}\hlopt{:}\hlkwd{ncol}\hlstd{(ytrain))}
  \hlstd{ytrain[, i]} \hlkwb{<-} \hlkwd{factor}\hlstd{(}\hlkwd{ifelse}\hlstd{(ytrain[, i]} \hlopt{==} \hlnum{1}\hlstd{,} \hlstr{"yes"}\hlstd{,} \hlstr{"no"}\hlstd{))}

\hlcom{# Save processed data to file}
\hlstd{data} \hlkwb{<-} \hlkwd{list}\hlstd{(}\hlkwc{train} \hlstd{= train,}
             \hlkwc{ytrain} \hlstd{= ytrain[,} \hlopt{-}\hlnum{1}\hlstd{],} \hlcom{# Drop id column }
             \hlkwc{test} \hlstd{= test,}
             \hlkwc{prop_missing_cutoff} \hlstd{= prop_missing_cutoff)}
\hlkwd{save}\hlstd{(data,} \hlkwc{file} \hlstd{=} \hlkwd{file.path}\hlstd{(data_dir,} \hlkwd{paste0}\hlstd{(}\hlstr{"data_cutoff"}\hlstd{, prop_missing_cutoff,} \hlstr{".rda"}\hlstd{)))}
\end{alltt}
\end{kframe}
\end{knitrout}
\end{document}
