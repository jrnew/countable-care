\documentclass{article}\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\usepackage{graphicx}
% Set page margins
\usepackage{geometry}
\geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
% Remove paragraph indenting
\setlength\parindent{0pt}
% Add hyperlinks
\usepackage{hyperref}
% Use math equations
\usepackage{amsmath}

\title{STAT 222: Project 4\\
       Countable Care}
\author{Yuan He, Lindsey Lee, Jin Rou New, Tianyi Zhu\\
        University of California, Berkeley}
\date{\today}

\setlength\parindent{0pt} % to remove paragraph indent
\setlength{\parskip}{\baselineskip} % to get space between paragraphs
\usepackage{setspace} % to allow for double line spacing



\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}
\maketitle

\begin{abstract}
  A Bayesian analysis of a randomized response survey was carried out to study the use of cognition-enhancing drugs in the UCB student population and gender-specific differences in this use. About three quarters of the student population do not use cognition-enhancing drugs, while about a tenth use them with a prescription and 14\% use them without a prescription. We concluded that there is no statistically significant difference in drug use between male and female students. \\ 
	\hfill\break
	\textbf{Keywords}: countable care, driven data competition, machine learning
\end{abstract}
\clearpage
%==================================================
\section{Introduction}
\label{sec:introduction}

The competition we have chosen to work on is DrivenData.org’s Countable Care: Modeling Women's Health Care Decisions. The link to the competition can be found at: http://www.drivendata.org/competitions/6/. DrivenData is the equivalent of Kaggle for social causes.
%==================================================
\section{Data}
\label{sec:data}

\subsection{Data description}
The data consists of responses by women in United States to the National Survey of Family Growth carried out by the United States Center for Disease Control and Prevention. Questions in the survey span topics such as demographics, marriage and reproductive health. There are a total of 14,644 observations, with each observation representing an individual’s responses for one release of the survey (an individual may have participated in multiple rounds of the survey), and a total of 1378 features, including the survey round and 116 numeric (standardized), 1050 categorical and 211 ordinal features that are responses to survey questions. The data has been obfuscated, so it is not given what each feature corresponds to in real terms.

An overview of the survey data is given in Table~\ref{tab-data}.

\begin{table}[ht]
  \centering
  \begin{tabular}{@{}|l|c|c|c|c|@{}}
  	\hline
		& Male & Female & Missing & Total\\ 
		\hline
		All observations & 62 & 46 & 9 & 117 \\ 
			Excluding fraternity observations & 41 & 45 & 8 & 94\\ 
		\hline
	\end{tabular}
	\caption{\textbf{Summary of the survey observations}} 
  \label{tab-data}
\end{table}

\subsection{Data problems}
The data is very sparse, with 83\% of the data missing in the train set. There are many features with a high proportion of missing values as shown in Fig~\ref{fig-missing}. This is because are many missing values as some survey questions depend on the response to previous survey questions and may be skipped.

\begin{figure}[htbp]
    \begin{center}
		\includegraphics[width = 0.4\textwidth]{fig/"prop-missing-in-columns".pdf}
		\caption{\textbf{Proportion of missing values in features.}}
		\label{fig-missing}
  \end{center}
\end{figure}

Secondly, columns of predictors/whether a women goes to a healthcare provider for different services may be dependent on each other. Fitting separate independent models for each health care service may result in a loss of information.

\subsection{Data processing}
\subsubsection{Imputing Missing Values}
The dataset was filled with missing values. More specifically, there were more missing values than valid values within the independent variables. Figure 1 showed that most columns were filled with missing values. In fact, 82.6% of the cells in train data were registered as "NA". Since columns with mostly missing values would do no good to the prediction but make the data redundant and slow to run, throwing away columns was considered before fitting the model. From Figure 1, most features had 10,000+ missing values out of 14,662 observations and could thus be thrown away. 

First of all, features that contain only missing values or one unique value (i.e. constant value for all women) were dropped from the data. There were 14 and 20 of such features respectively.

Secondly, features with missing values exceeding a threshold would be dropped. If the threshold was set to be 50%, i.e. columns with proportion of NA's exceeding 50% would be considered invalid, then 1159 out of 1378 independent variables would be discarded. To compare, an 80% cutoff threw away 1038 out of 1378 columns. In practice we experimented with both cutoffs for each model and compared the accuracy resulted from loss of data.

Lastly, after removing invalid columns, NA values left were handled using different approaches for different data types (categorical, numeric or ordinal). Since more than one data type was presented in the independent variables, plus that the dataset was huge and obfuscated, it was hard to use predictions from regression to fill in the NA values. Instead, generic methods were used for each datatype:
1. For numeric columns, NA's were converted to 0's;
2. For ordinal columns, NA's were converted to -1;
3. For categorical columns, new category called "missing" was introduced.

\subsubsection{Introducing Dummy Variables}
In order to fit certain algorithms (e.g. Ridge, SVM, logit), columns with data type "categorical" were required to be made into dummy variables. Function model.matrix in R was used to introduce dummy columns with categorical columns treated as factors.
Levels of each categorical variable were compared for train data and test data. Additional levels in test data that did not appear in train data were removed to make sure the models were trained properly. For levels that appeared in train data but not in test data, placeholder columns (columns with all 0's) were introduced to test data to take up the position of that level. This was necessary because it was required that train data and test data have the same number of columns, even after introducing dummy variables.

%==================================================
\section{Methods}
\label{sec:methods}
\subsection{Feature engineering}
We engineered a number of features, including:
\begin{itemize}
  \item Number of numeric features with missing values for each woman
  \item Number of ordinal features with missing values for each woman
  \item Number of categorical features with missing values for each woman
\end{itemize}

\subsection{Models}
We fitted a variety of models, including:
\begin{itemize}
  \item Gradient Boosting Machine (GBM)
\end{itemize}

\subsection{Computation}
All calculations were carried out in the \texttt{R} programming language, with additional functions from the \texttt{caret} package. Annotated code is given in the Appendix in Section \ref{sec:appendix}.
%==================================================
\section{Results} 
\label{sec:results}

\subsection{Prediction objective and evaluation metric}
Binary outcomes denoting if a woman went to a healthcare provider for each of 14 services in the 12 months preceding the survey are given in the training set. The prediction objective is to predict the probability for each of the 3,661 women in the test set of going to a healthcare provider for each of the 14 services. In other words, 3,661 x 14 predicted probabilities are required. The 14 services are not mutually exclusive; a woman can go to more than one service in the 12 months.

The evaluation metric here is the logarithmic loss, defined by $-\frac{1}{n}$. (FILL IN!)

\subsection{Prediction results}
A summary of the performance of our models on the test set is given in Table~\ref{tab-results}.

\begin{table}[ht]
  \centering
  \begin{tabular}{@{}|l|c|c|@{}}
    \hline
  	Data set & Model & Log loss\\ 
		\hline
  	Missing value cut-off of 50\% & Gradient boosting method & 0.2796 \\ 
    Missing value cut-off of 80\% & Gradient boosting method & 0.2812 \\
  	\hline
	\end{tabular}
	\caption{\textbf{Summary of model performance on test set.}}
  \label{tab-results}
\end{table}
%==================================================
\section{Discussion} 
\label{sec:discussion}

\subsection{Shortcomings and Future Developments}
One significant feature about this dataset was that it had not one, but fourteen dependent variables (Y's). And the dependent variables were correlated with each other since a woman usually orders more than one health services. Conventional regression models assumed the dimension of Y to be nx1, so did the algorithms in R. Therefore, 14 columns of data were fitted separately using the models, assuming they were independent, which was not actually true. Ignoring the correlation between Y variables may cause loss of accuracy. Application of multivariate methods might be able to increase prediction accuracy.

Methodology of imputing missing values might also result in loss of accuracy. Throwing away columns was a straightforward, but not elegant way to reduce dimension and save computer time. Ideally only columns with all NA's and columns with a single constant should be removed. Moreover, replacing missing values with a global value (e.g. 0 or -1) was a compromise due to the large amount of NA's and obfuscation of dataset. Provided more valid data and smaller size, regressing the missing values on other columns should be a better solution.

Procedure of creating dummy variables could be another source of problem. First of all, some levels in test data had to be dropped (these levels were replaced by "missing"). Secondly, introducing dummy variables dramatically increased dimension of data, without bringing in extra information. Lastly, the sole purpose of introducing placeholder columns (columns with only 0's) in test data was to make the number of columns match for test and train. Given that the model permitted, using factors instead of dummy variables should be more concise.

When regression models yielded prediction for probabilities, some of them might fall out of the range of 0 to 1. Converting these over-bound values to 0 or 1 would dramatically increase the score since log-loss was used for evaluation of prediction. To avoid this problem, values over-bound were replaced with column means. However, a finer way should be adopted to accommodate those values.  

%==================================================
\section{References}
\clearpage
%==================================================
\appendix
\section*{Appendix}
\label{sec:appendix}


\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{rm}\hlstd{(}\hlkwc{list} \hlstd{=} \hlkwd{ls}\hlstd{())}
\hlkwd{gc}\hlstd{()}
\hlstd{run_on_server} \hlkwb{<-} \hlnum{TRUE}
\hlkwa{if} \hlstd{(}\hlopt{!}\hlstd{run_on_server)}
  \hlkwd{setwd}\hlstd{(}\hlstr{"~/Copy/Berkeley/stat222-spring-2015/stat222sp15/projects/countable-care"}\hlstd{)}
\hlstd{data_dir} \hlkwb{<-} \hlstr{"data"}
\hlstd{fig_dir} \hlkwb{<-} \hlstr{"fig"}
\hlstd{results_dir} \hlkwb{<-} \hlstr{"results"}
\hlkwd{dir.create}\hlstd{(fig_dir,} \hlkwc{showWarnings} \hlstd{=} \hlnum{FALSE}\hlstd{)}
\hlkwd{dir.create}\hlstd{(results_dir,} \hlkwc{showWarnings} \hlstd{=} \hlnum{FALSE}\hlstd{)}
\hlkwd{dir.create}\hlstd{(}\hlstr{"submit"}\hlstd{,} \hlkwc{showWarnings} \hlstd{=} \hlnum{FALSE}\hlstd{)}
\hlstd{get_notifications} \hlkwb{<-} \hlkwd{ifelse}\hlstd{(run_on_server,} \hlnum{TRUE}\hlstd{,} \hlnum{FALSE}\hlstd{)}
\hlkwa{if} \hlstd{(get_notifications) \{}
  \hlkwd{library}\hlstd{(RPushbullet)}
  \hlcom{# options(error = function() \{ # Be notified when there is an error}
  \hlcom{#   pbPost("note", "Error!", geterrmessage(), recipients = c(1, 2))}
  \hlcom{# \})}
\hlstd{\}}

\hlstd{write_submission} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{probs}\hlstd{,} \hlkwc{model_name}\hlstd{) \{}
  \hlstd{file_path} \hlkwb{<-} \hlkwd{file.path}\hlstd{(}\hlstr{"submit"}\hlstd{,} \hlkwd{paste0}\hlstd{(model_name,} \hlstr{".csv"}\hlstd{))}
  \hlstd{submit} \hlkwb{<-} \hlkwd{read.csv}\hlstd{(}\hlstr{"data/SubmissionFormat.csv"}\hlstd{)}
  \hlstd{submit[,} \hlnum{2}\hlopt{:}\hlkwd{ncol}\hlstd{(submit)]} \hlkwb{<-} \hlstd{probs}
  \hlkwa{if} \hlstd{(}\hlkwd{file.exists}\hlstd{(file_path))}
    \hlkwd{stop}\hlstd{(}\hlkwd{paste0}\hlstd{(file_path,} \hlstr{" already exists!"}\hlstd{))}
  \hlkwd{write.csv}\hlstd{(submit,} \hlkwd{file.path}\hlstd{(file_path),} \hlkwc{row.names} \hlstd{=} \hlnum{FALSE}\hlstd{)}
  \hlkwd{message}\hlstd{(}\hlkwd{paste0}\hlstd{(}\hlstr{"Results written to "}\hlstd{, file_path))}
\hlstd{\}}

\hlstd{seed} \hlkwb{<-} \hlnum{12345}
\hlkwd{set.seed}\hlstd{(seed)}
\hlkwd{library}\hlstd{(caret)}
\hlkwd{library}\hlstd{(e1071)}
\hlcom{# List all models in caret}
\hlcom{# names(getModelInfo())}

\hlcom{# Load data}
\hlstd{prop_missing_cutoff} \hlkwb{<-} \hlnum{0.5}
\hlkwd{load}\hlstd{(}\hlkwc{file} \hlstd{=} \hlkwd{file.path}\hlstd{(data_dir,} \hlkwd{paste0}\hlstd{(}\hlstr{"data_cutoff"}\hlstd{, prop_missing_cutoff,} \hlstr{".rda"}\hlstd{)))}
\hlstd{train} \hlkwb{<-} \hlstd{data}\hlopt{$}\hlstd{train}
\hlstd{test} \hlkwb{<-} \hlstd{data}\hlopt{$}\hlstd{test}
\hlstd{ytrain} \hlkwb{<-} \hlstd{data}\hlopt{$}\hlstd{ytrain}

\hlcom{# Create data partitions of 80% and 20%}
\hlstd{ntrain} \hlkwb{<-} \hlkwd{nrow}\hlstd{(train)}
\hlstd{train_indices} \hlkwb{<-} \hlkwd{sample}\hlstd{(}\hlnum{1}\hlopt{:}\hlstd{ntrain)[}\hlnum{1}\hlopt{:}\hlkwd{floor}\hlstd{(ntrain}\hlopt{*}\hlnum{0.8}\hlstd{)]}
\hlstd{train_val} \hlkwb{<-} \hlstd{train[}\hlopt{-}\hlstd{train_indices, ]}

\hlcom{# Set up caret models}
\hlstd{train_control} \hlkwb{<-} \hlkwd{trainControl}\hlstd{(}\hlkwc{method} \hlstd{=} \hlstr{"cv"}\hlstd{,} \hlkwc{number} \hlstd{=} \hlnum{10}\hlstd{,} \hlkwc{returnResamp} \hlstd{=} \hlstr{"none"}\hlstd{)}

\hlstd{mod_types} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlstr{"gbm"}\hlstd{,} \hlstr{"rf"}\hlstd{)}
\hlstd{mod} \hlkwb{<-} \hlkwd{list}\hlstd{()}
\hlstd{probs} \hlkwb{<-} \hlkwd{matrix}\hlstd{(}\hlnum{NA}\hlstd{,} \hlkwd{nrow}\hlstd{(test),} \hlkwd{ncol}\hlstd{(ytrain))}
\hlkwa{for} \hlstd{(mod_type} \hlkwa{in} \hlstd{mod_types) \{}
  \hlkwa{for} \hlstd{(svc_index} \hlkwa{in} \hlnum{1}\hlopt{:}\hlkwd{ncol}\hlstd{(ytrain)) \{}

    \hlcom{# Testing!!!}
    \hlcom{# mod_type <- "knn"}
    \hlcom{# train_indices <- 1:100}
    \hlcom{# svc_index <- 1}

    \hlcom{# Train all the models with train data}
    \hlstd{mod[[svc_index]]} \hlkwb{<-} \hlkwd{train}\hlstd{(train[train_indices, ], ytrain[train_indices, svc_index],}
                              \hlkwc{method} \hlstd{= mod_type,} \hlkwc{trControl} \hlstd{= train_control)}

    \hlcom{# Predict on test data}
    \hlstd{probs[, svc_index]} \hlkwb{<-} \hlkwd{predict}\hlstd{(}\hlkwc{object} \hlstd{= mod[[svc_index]],} \hlkwc{newdata} \hlstd{= test,}
                                  \hlkwc{type} \hlstd{=} \hlstr{"prob"}\hlstd{)}\hlopt{$}\hlstd{yes}

    \hlcom{# Get predictions for each model and add them back to themselves}
    \hlcom{# train_val[[paste0(mod_type, "_PROB"]] <- predict(mod[[svc_index]], train_val, type = "prob")}
    \hlcom{# test[[paste0(mod_type, "_PROB"]] <- predict(mod[[svc_index]], test, type = "prob")}

    \hlcom{# Run an ensemble model to blend all the predicted probabilities}
    \hlcom{# mod_ensemble[[svc_index]] <- train(train_val, ytrain[-train_indices, svc_index], }
    \hlcom{#                                    method = "lasso", trControl = train_control)}

    \hlcom{# Predict on test data}
    \hlcom{# preds <- predict(mod_ensemble[[svc_index]], test, type = "prob")}
  \hlstd{\}}
  \hlkwd{write_submission}\hlstd{(probs,} \hlkwd{paste0}\hlstd{(mod_type,} \hlstr{"_cutoff"}\hlstd{, prop_missing_cutoff))}
  \hlkwd{save}\hlstd{(mod,} \hlkwc{file} \hlstd{=} \hlkwd{file.path}\hlstd{(results_dir,}
                             \hlkwd{paste0}\hlstd{(}\hlstr{"mod_"}\hlstd{, mod_type,} \hlstr{"_cutoff"}\hlstd{, prop_missing_cutoff,} \hlstr{".rda"}\hlstd{)))}
  \hlkwa{if} \hlstd{(get_notifications)}
    \hlkwd{pbPost}\hlstd{(}\hlkwc{type} \hlstd{=} \hlstr{"note"}\hlstd{,}
           \hlkwc{title} \hlstd{=} \hlstr{"stat222"}\hlstd{,}
           \hlkwc{body} \hlstd{=} \hlkwd{paste0}\hlstd{(mod_type,} \hlstr{" done!"}\hlstd{),}
           \hlkwc{recipients} \hlstd{=} \hlkwd{c}\hlstd{(}\hlnum{1}\hlstd{,} \hlnum{2}\hlstd{))}
\hlstd{\}}
\end{alltt}
\end{kframe}
\end{knitrout}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{rm}\hlstd{(}\hlkwc{list} \hlstd{=} \hlkwd{ls}\hlstd{())}
\hlkwd{gc}\hlstd{()}
\hlkwd{setwd}\hlstd{(}\hlstr{"~/Copy/Berkeley/stat222-spring-2015/stat222sp15/projects/countable-care"}\hlstd{)}
\hlstd{data_dir} \hlkwb{<-} \hlstr{"data"}
\hlstd{fig_dir} \hlkwb{<-} \hlstr{"fig"}
\hlstd{results_dir} \hlkwb{<-} \hlstr{"results"}
\hlkwd{dir.create}\hlstd{(fig_dir,} \hlkwc{showWarnings} \hlstd{=} \hlnum{FALSE}\hlstd{)}
\hlkwd{dir.create}\hlstd{(results_dir,} \hlkwc{showWarnings} \hlstd{=} \hlnum{FALSE}\hlstd{)}
\hlkwd{dir.create}\hlstd{(}\hlstr{"submit"}\hlstd{,} \hlkwc{showWarnings} \hlstd{=} \hlnum{FALSE}\hlstd{)}
\hlstd{prop_missing_cutoff} \hlkwb{<-} \hlnum{0.9}

\hlcom{# Read in data}
\hlstd{train_readin} \hlkwb{<-} \hlkwd{read.csv}\hlstd{(}\hlkwd{file.path}\hlstd{(data_dir,} \hlstr{"train_values.csv"}\hlstd{),}
                         \hlkwc{stringsAsFactors} \hlstd{=} \hlnum{FALSE}\hlstd{,} \hlkwc{na.strings} \hlstd{=} \hlstr{""}\hlstd{)}
\hlstd{ytrain} \hlkwb{<-} \hlkwd{read.csv}\hlstd{(}\hlkwd{file.path}\hlstd{(data_dir,} \hlstr{"train_labels.csv"}\hlstd{))}
\hlstd{test_readin} \hlkwb{<-} \hlkwd{read.csv}\hlstd{(}\hlkwd{file.path}\hlstd{(data_dir,} \hlstr{"test_values.csv"}\hlstd{),}
                        \hlkwc{stringsAsFactors} \hlstd{=} \hlnum{FALSE}\hlstd{,} \hlkwc{na.strings} \hlstd{=} \hlstr{""}\hlstd{)}

\hlcom{# Data exploration}
\hlcom{# Column types}
\hlstd{colnames_all} \hlkwb{<-} \hlkwd{names}\hlstd{(train_readin)}
\hlstd{colnames_type} \hlkwb{<-} \hlkwd{sapply}\hlstd{(colnames_all,} \hlkwa{function}\hlstd{(}\hlkwc{x}\hlstd{)} \hlkwd{strsplit}\hlstd{(x,} \hlstr{"_"}\hlstd{)[[}\hlnum{1}\hlstd{]][}\hlnum{1}\hlstd{])}
\hlkwd{table}\hlstd{(colnames_type[}\hlopt{-}\hlkwd{c}\hlstd{(}\hlnum{1}\hlstd{,} \hlnum{2}\hlstd{)])}

\hlcom{# Check proportion of missing values in features}
\hlstd{prop_missing} \hlkwb{<-} \hlkwd{sapply}\hlstd{(train_readin,} \hlkwa{function}\hlstd{(}\hlkwc{x}\hlstd{)} \hlkwd{mean}\hlstd{(}\hlkwd{is.na}\hlstd{(x)))}
\hlkwd{par}\hlstd{(}\hlkwc{mar} \hlstd{=} \hlkwd{c}\hlstd{(}\hlnum{4.5}\hlstd{,} \hlnum{4.5}\hlstd{,} \hlnum{4.5}\hlstd{,} \hlnum{2}\hlstd{))}
\hlkwd{pdf}\hlstd{(}\hlkwd{file.path}\hlstd{(fig_dir,} \hlstr{"prop-missing-in-columns.pdf"}\hlstd{),} \hlkwc{width} \hlstd{=} \hlnum{7}\hlstd{,} \hlkwc{height} \hlstd{=} \hlnum{5}\hlstd{)}
\hlkwd{hist}\hlstd{(prop_missing,} \hlkwc{main} \hlstd{=} \hlstr{"Proportion of missing values\textbackslash{}nin features"}\hlstd{,}
     \hlkwc{xlab} \hlstd{=} \hlstr{"Proportion"}\hlstd{,} \hlkwc{ylab} \hlstd{=} \hlstr{"Number of features"}\hlstd{,} \hlkwc{col} \hlstd{=} \hlstr{"lightgrey"}\hlstd{)}
\hlkwd{dev.off}\hlstd{()}

\hlcom{# Missing pattern plot}
\hlcom{# library(mi)}
\hlcom{# missing.pattern.plot(train)}

\hlcom{# Data processing}
\hlcom{# Check original number of features, not including id}
\hlkwd{ncol}\hlstd{(train_readin[,} \hlopt{-}\hlnum{1}\hlstd{])} \hlcom{# 1378}

\hlcom{# Check for features with only constant values}
\hlstd{cols_constant} \hlkwb{<-} \hlkwd{sapply}\hlstd{(train_readin,} \hlkwa{function}\hlstd{(}\hlkwc{x}\hlstd{)} \hlkwd{length}\hlstd{(}\hlkwd{unique}\hlstd{(x))} \hlopt{==} \hlnum{1}\hlstd{)}
\hlkwd{sum}\hlstd{(cols_constant)} \hlcom{# 20}
\hlcom{# names(train_readin)[cols_constant]}

\hlcom{# Feature engineering}
\hlcom{# Column types}
\hlstd{colnames_all} \hlkwb{<-} \hlkwd{names}\hlstd{(train_readin)}
\hlstd{colnames_type} \hlkwb{<-} \hlkwd{sapply}\hlstd{(colnames_all,} \hlkwa{function}\hlstd{(}\hlkwc{x}\hlstd{)} \hlkwd{strsplit}\hlstd{(x,} \hlstr{"_"}\hlstd{)[[}\hlnum{1}\hlstd{]][}\hlnum{1}\hlstd{])}
\hlkwd{table}\hlstd{(colnames_type[}\hlopt{-}\hlkwd{c}\hlstd{(}\hlnum{1}\hlstd{,} \hlnum{2}\hlstd{)])}
\hlstd{cols_numeric} \hlkwb{<-} \hlstd{colnames_type} \hlopt{==} \hlstr{"n"}
\hlstd{cols_ordinal} \hlkwb{<-} \hlstd{colnames_type} \hlopt{==} \hlstr{"o"}
\hlstd{cols_categorical} \hlkwb{<-} \hlstd{colnames_type} \hlopt{==} \hlstr{"c"}

\hlcom{# Number of missing numeric/ordinal/categorical features}
\hlstd{num_missing_numeric} \hlkwb{<-} \hlkwd{apply}\hlstd{(train_readin[, cols_numeric],} \hlnum{1}\hlstd{,} \hlkwa{function}\hlstd{(}\hlkwc{x}\hlstd{)} \hlkwd{sum}\hlstd{(}\hlkwd{is.na}\hlstd{(x)))}
\hlstd{num_missing_ordinal} \hlkwb{<-} \hlkwd{apply}\hlstd{(train_readin[, cols_ordinal],} \hlnum{1}\hlstd{,} \hlkwa{function}\hlstd{(}\hlkwc{x}\hlstd{)} \hlkwd{sum}\hlstd{(}\hlkwd{is.na}\hlstd{(x)))}
\hlstd{num_missing_categorical} \hlkwb{<-} \hlkwd{apply}\hlstd{(train_readin[, cols_categorical],} \hlnum{1}\hlstd{,} \hlkwa{function}\hlstd{(}\hlkwc{x}\hlstd{)} \hlkwd{sum}\hlstd{(}\hlkwd{is.na}\hlstd{(x)))}
\hlkwd{hist}\hlstd{(num_missing_numeric,} \hlkwc{freq} \hlstd{=} \hlnum{FALSE}\hlstd{,}
     \hlkwc{main} \hlstd{=} \hlstr{"Distribution of missing\textbackslash{}nnumeric features across all women"}\hlstd{,}
     \hlkwc{xlab} \hlstd{=} \hlstr{"Number of missing numeric features"}\hlstd{)}
\hlkwd{hist}\hlstd{(num_missing_ordinal,} \hlkwc{freq} \hlstd{=} \hlnum{FALSE}\hlstd{,}
     \hlkwc{main} \hlstd{=} \hlstr{"Distribution of missing\textbackslash{}nordinal features across all women"}\hlstd{,}
     \hlkwc{xlab} \hlstd{=} \hlstr{"Number of missing ordinal features"}\hlstd{)}
\hlkwd{hist}\hlstd{(num_missing_categorical,} \hlkwc{freq} \hlstd{=} \hlnum{FALSE}\hlstd{,}
     \hlkwc{main} \hlstd{=} \hlstr{"Distribution of missing\textbackslash{}ncategorical features across all women"}\hlstd{,}
     \hlkwc{xlab} \hlstd{=} \hlstr{"Number of missing categorical features"}\hlstd{)}
\hlstd{num_missing_numeric_test} \hlkwb{<-} \hlkwd{apply}\hlstd{(test_readin[, cols_numeric],} \hlnum{1}\hlstd{,} \hlkwa{function}\hlstd{(}\hlkwc{x}\hlstd{)} \hlkwd{sum}\hlstd{(}\hlkwd{is.na}\hlstd{(x)))}
\hlstd{num_missing_ordinal_test} \hlkwb{<-} \hlkwd{apply}\hlstd{(test_readin[, cols_ordinal],} \hlnum{1}\hlstd{,} \hlkwa{function}\hlstd{(}\hlkwc{x}\hlstd{)} \hlkwd{sum}\hlstd{(}\hlkwd{is.na}\hlstd{(x)))}
\hlstd{num_missing_categorical_test} \hlkwb{<-} \hlkwd{apply}\hlstd{(test_readin[, cols_categorical],} \hlnum{1}\hlstd{,} \hlkwa{function}\hlstd{(}\hlkwc{x}\hlstd{)} \hlkwd{sum}\hlstd{(}\hlkwd{is.na}\hlstd{(x)))}

\hlcom{# Check for features with proportion of missing values > prop_missing_cutoff}
\hlstd{prop_missing} \hlkwb{<-} \hlkwd{sapply}\hlstd{(train_readin,} \hlkwa{function}\hlstd{(}\hlkwc{x}\hlstd{)} \hlkwd{mean}\hlstd{(}\hlkwd{is.na}\hlstd{(x)))}
\hlstd{cols_missing} \hlkwb{<-} \hlstd{prop_missing} \hlopt{>} \hlstd{prop_missing_cutoff}
\hlkwd{sum}\hlstd{(cols_missing)} \hlcom{# 1159 for 0.5, 1038 for 0.8}

\hlcom{# Remaining number of features, not including id}
\hlkwd{sum}\hlstd{(}\hlopt{!}\hlstd{cols_constant} \hlopt{& !}\hlstd{cols_missing)} \hlopt{-} \hlnum{1} \hlcom{# 213 for 0.5, 334 for 0.8}

\hlcom{# Remove above features and id}
\hlstd{train} \hlkwb{<-} \hlstd{train_readin[,} \hlkwd{names}\hlstd{(train_readin)} \hlopt{!=} \hlstr{"id"} \hlopt{& !}\hlstd{cols_constant} \hlopt{& !}\hlstd{cols_missing]}
\hlstd{test} \hlkwb{<-} \hlstd{test_readin[,} \hlkwd{names}\hlstd{(train_readin)} \hlopt{!=} \hlstr{"id"} \hlopt{& !}\hlstd{cols_constant} \hlopt{& !}\hlstd{cols_missing]}

\hlcom{# Column types}
\hlstd{colnames_all} \hlkwb{<-} \hlkwd{names}\hlstd{(train)}
\hlstd{colnames_type} \hlkwb{<-} \hlkwd{sapply}\hlstd{(colnames_all,} \hlkwa{function}\hlstd{(}\hlkwc{x}\hlstd{)} \hlkwd{strsplit}\hlstd{(x,} \hlstr{"_"}\hlstd{)[[}\hlnum{1}\hlstd{]][}\hlnum{1}\hlstd{])}
\hlkwd{table}\hlstd{(colnames_type[}\hlopt{-}\hlkwd{c}\hlstd{(}\hlnum{1}\hlstd{,} \hlnum{2}\hlstd{)])}
\hlstd{cols_numeric} \hlkwb{<-} \hlstd{colnames_type} \hlopt{==} \hlstr{"n"}
\hlstd{cols_ordinal} \hlkwb{<-} \hlstd{colnames_type} \hlopt{==} \hlstr{"o"}
\hlstd{cols_categorical} \hlkwb{<-} \hlstd{colnames_type} \hlopt{==} \hlstr{"c"}

\hlcom{# Missing value imputation for remaining features}
\hlcom{# Numeric features: Set as 0}
\hlkwa{for} \hlstd{(i} \hlkwa{in} \hlnum{1}\hlopt{:}\hlkwd{sum}\hlstd{(cols_numeric)) \{}
  \hlstd{train[, cols_numeric][, i]} \hlkwb{<-} \hlkwd{ifelse}\hlstd{(}\hlkwd{is.na}\hlstd{(train[, cols_numeric][, i]),}
                                       \hlnum{0}\hlstd{, train[, cols_numeric][, i])}
  \hlstd{test[, cols_numeric][, i]} \hlkwb{<-} \hlkwd{ifelse}\hlstd{(}\hlkwd{is.na}\hlstd{(test[, cols_numeric][, i]),}
                                      \hlnum{0}\hlstd{, test[, cols_numeric][, i])}
\hlstd{\}}
\hlcom{# Ordinal features: Set as -1}
\hlcom{# table(sapply(train[, cols_ordinal], min, na.rm = TRUE)) # min is 0 or 1}
\hlkwa{for} \hlstd{(i} \hlkwa{in} \hlnum{1}\hlopt{:}\hlkwd{sum}\hlstd{(cols_ordinal)) \{}
  \hlstd{train[, cols_ordinal][, i]} \hlkwb{<-} \hlkwd{as.integer}\hlstd{(}\hlkwd{ifelse}\hlstd{(}\hlkwd{is.na}\hlstd{(train[, cols_ordinal][, i]),}
                                              \hlopt{-}\hlnum{1}\hlstd{, train[, cols_ordinal][, i]))}
  \hlstd{test[, cols_ordinal][, i]} \hlkwb{<-} \hlkwd{as.integer}\hlstd{(}\hlkwd{ifelse}\hlstd{(}\hlkwd{is.na}\hlstd{(test[, cols_ordinal][, i]),}
                                             \hlopt{-}\hlnum{1}\hlstd{, test[, cols_ordinal][, i]))}
\hlstd{\}}

\hlcom{# Categorical features}
\hlcom{# Check for: i) features with categories in test but not train set.}
\hlcom{# ii) features with missing values in test but not train set}
\hlstd{cols_unknownlevels} \hlkwb{<-} \hlkwa{NULL}
\hlstd{cols_nomissingintrain} \hlkwb{<-} \hlkwa{NULL}
\hlkwa{for} \hlstd{(i} \hlkwa{in} \hlnum{1}\hlopt{:}\hlkwd{sum}\hlstd{(cols_categorical)) \{}
  \hlkwa{if} \hlstd{(}\hlkwd{any}\hlstd{(}\hlkwd{is.na}\hlstd{(test[, cols_categorical][, i]))} \hlopt{&} \hlkwd{all}\hlstd{(}\hlopt{!}\hlkwd{is.na}\hlstd{(train[, cols_categorical][, i]))) \{}
    \hlkwd{print}\hlstd{(i)}
    \hlstd{cols_nomissingintrain} \hlkwb{<-} \hlkwd{c}\hlstd{(cols_nomissingintrain, i)}
  \hlstd{\}}
  \hlkwa{if} \hlstd{(}\hlkwd{any}\hlstd{(}\hlopt{!}\hlkwd{is.na}\hlstd{(test[, cols_categorical][, i])} \hlopt{&}
            \hlopt{!}\hlstd{(test[, cols_categorical][, i]} \hlopt{%in%} \hlkwd{unique}\hlstd{(train[, cols_categorical][, i])))) \{}
    \hlstd{cols_unknownlevels} \hlkwb{<-} \hlkwd{c}\hlstd{(cols_unknownlevels, i)}
    \hlstd{levels_train} \hlkwb{<-} \hlkwd{unique}\hlstd{(train[, cols_categorical][, i])}
    \hlstd{levels_train} \hlkwb{<-} \hlkwd{ifelse}\hlstd{(}\hlkwd{is.na}\hlstd{(levels_train),} \hlstr{"missing"}\hlstd{,} \hlkwd{as.character}\hlstd{(levels_train))}
    \hlstd{levels_test} \hlkwb{<-} \hlkwd{unique}\hlstd{(test[, cols_categorical][, i])}
    \hlstd{levels_test} \hlkwb{<-} \hlkwd{ifelse}\hlstd{(}\hlkwd{is.na}\hlstd{(levels_test),} \hlstr{"missing"}\hlstd{,} \hlkwd{as.character}\hlstd{(levels_test))}
    \hlkwa{if} \hlstd{(}\hlopt{!}\hlstd{(}\hlstr{"missing"} \hlopt{%in%} \hlstd{levels_train)) \{}
      \hlkwd{print}\hlstd{(i)}
      \hlkwd{print}\hlstd{(levels_test[}\hlopt{!}\hlstd{(levels_test} \hlopt{%in%} \hlstd{levels_train)])}
      \hlkwd{print}\hlstd{(}\hlstr{"---"}\hlstd{)}
      \hlstd{cols_nomissingintrain} \hlkwb{<-} \hlkwd{c}\hlstd{(cols_nomissingintrain,}
                                 \hlkwd{rep}\hlstd{(i,} \hlkwd{length}\hlstd{(levels_test[}\hlopt{!}\hlstd{(levels_test} \hlopt{%in%} \hlstd{levels_train)])))}
    \hlstd{\}}
  \hlstd{\}}
\hlstd{\}}

\hlcom{# Categorical features: Set as new category missing}
\hlkwa{for} \hlstd{(i} \hlkwa{in} \hlnum{1}\hlopt{:}\hlkwd{sum}\hlstd{(cols_categorical)) \{}
  \hlstd{train[, cols_categorical][, i]} \hlkwb{<-} \hlkwd{as.factor}\hlstd{(}\hlkwd{ifelse}\hlstd{(}\hlkwd{is.na}\hlstd{(train[, cols_categorical][, i]),}
                                                  \hlstr{"missing"}\hlstd{, train[, cols_categorical][, i]))}
  \hlstd{test[, cols_categorical][, i]} \hlkwb{<-} \hlkwd{as.factor}\hlstd{(}\hlkwd{ifelse}\hlstd{(}\hlkwd{is.na}\hlstd{(test[, cols_categorical][, i]),}
                                                    \hlstr{"missing"}\hlstd{,}
                                                    \hlkwd{ifelse}\hlstd{(}\hlopt{!}\hlstd{(test[, cols_categorical][, i]} \hlopt{%in%}
                                                      \hlkwd{unique}\hlstd{(train[, cols_categorical][, i])),}
                                                      \hlstr{"missing"}\hlstd{, test[, cols_categorical][, i])))}
\hlstd{\}}

\hlcom{# Set values in test set to most frequently-occurring category in train set for:}
\hlcom{# i) features with categories in test but not train set.}
\hlcom{# ii) features with missing values in test but not train set}
\hlkwa{for} \hlstd{(c} \hlkwa{in} \hlkwd{seq_along}\hlstd{(cols_nomissingintrain)) \{}
  \hlstd{i} \hlkwb{<-} \hlstd{cols_nomissingintrain[c]}
  \hlstd{value_new} \hlkwb{<-} \hlkwd{names}\hlstd{(}\hlkwd{which.max}\hlstd{(}\hlkwd{table}\hlstd{(test[, cols_categorical][, i])))}
  \hlstd{test[, cols_categorical][, i]} \hlkwb{<-} \hlkwd{as.factor}\hlstd{(}\hlkwd{ifelse}\hlstd{(}\hlkwd{as.character}\hlstd{(test[, cols_categorical][, i])} \hlopt{==}
                                                      \hlstr{"missing"}\hlstd{,}
                                                    \hlstd{value_new,}
                                                    \hlkwd{as.character}\hlstd{(test[, cols_categorical][, i])))}
\hlstd{\}}

\hlcom{# Add engineered features to data}
\hlstd{train}\hlopt{$}\hlstd{num_missing_numeric} \hlkwb{<-} \hlstd{num_missing_numeric}
\hlstd{train}\hlopt{$}\hlstd{num_missing_ordinal} \hlkwb{<-} \hlstd{num_missing_ordinal}
\hlstd{train}\hlopt{$}\hlstd{num_missing_categorical} \hlkwb{<-} \hlstd{num_missing_categorical}
\hlstd{test}\hlopt{$}\hlstd{num_missing_numeric} \hlkwb{<-} \hlstd{num_missing_numeric_test}
\hlstd{test}\hlopt{$}\hlstd{num_missing_ordinal} \hlkwb{<-} \hlstd{num_missing_ordinal_test}
\hlstd{test}\hlopt{$}\hlstd{num_missing_categorical} \hlkwb{<-} \hlstd{num_missing_categorical_test}

\hlcom{# Convert release variable to factor, else it throws error}
\hlstd{train}\hlopt{$}\hlstd{release} \hlkwb{<-} \hlkwd{as.factor}\hlstd{(train}\hlopt{$}\hlstd{release)}
\hlstd{test}\hlopt{$}\hlstd{release} \hlkwb{<-} \hlkwd{as.factor}\hlstd{(test}\hlopt{$}\hlstd{release)}

\hlcom{# Convert prediction label to alphabetical factor, }
\hlcom{# else it throws error in caret::predict}
\hlkwa{for} \hlstd{(i} \hlkwa{in} \hlnum{1}\hlopt{:}\hlkwd{ncol}\hlstd{(ytrain))}
  \hlstd{ytrain[, i]} \hlkwb{<-} \hlkwd{factor}\hlstd{(}\hlkwd{ifelse}\hlstd{(ytrain[, i]} \hlopt{==} \hlnum{1}\hlstd{,} \hlstr{"yes"}\hlstd{,} \hlstr{"no"}\hlstd{))}

\hlcom{# Save processed data to file}
\hlstd{data} \hlkwb{<-} \hlkwd{list}\hlstd{(}\hlkwc{train} \hlstd{= train,}
             \hlkwc{ytrain} \hlstd{= ytrain[,} \hlopt{-}\hlnum{1}\hlstd{],} \hlcom{# Drop id column }
             \hlkwc{test} \hlstd{= test,}
             \hlkwc{prop_missing_cutoff} \hlstd{= prop_missing_cutoff)}
\hlkwd{save}\hlstd{(data,} \hlkwc{file} \hlstd{=} \hlkwd{file.path}\hlstd{(data_dir,} \hlkwd{paste0}\hlstd{(}\hlstr{"data_cutoff"}\hlstd{, prop_missing_cutoff,} \hlstr{".rda"}\hlstd{)))}
\end{alltt}
\end{kframe}
\end{knitrout}
\end{document}
