\documentclass{article}\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\usepackage{graphicx}
% Set page margins
\usepackage{geometry}
\geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
% Remove paragraph indenting
\setlength\parindent{0pt}
% Add hyperlinks
\usepackage{hyperref}
% Use math equations
\usepackage{amsmath}

\title{STAT 222: Project 4\\
       Countable Care}
\author{Yuan He, Lindsey Lee, Jin Rou New, Tianyi Zhu\\
        University of California, Berkeley}
\date{\today}

\setlength\parindent{0pt} % to remove paragraph indent
\setlength{\parskip}{\baselineskip} % to get space between paragraphs
\usepackage{setspace} % to allow for double line spacing



\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}
\maketitle

\begin{abstract}
  A Bayesian analysis of a randomized response survey was carried out to study the use of cognition-enhancing drugs in the UCB student population and gender-specific differences in this use. About three quarters of the student population do not use cognition-enhancing drugs, while about a tenth use them with a prescription and 14\% use them without a prescription. We concluded that there is no statistically significant difference in drug use between male and female students. \\ 
	\hfill\break
	\textbf{Keywords}: countable care, driven data competition, machine learning
\end{abstract}
\clearpage
%==================================================
\section{Introduction}
\label{sec:introduction}

The competition we have chosen to work on is DrivenData.org’s Countable Care: Modeling Women's Health Care Decisions. The link to the competition can be found at: http://www.drivendata.org/competitions/6/. DrivenData is the equivalent of Kaggle for social causes.
%==================================================
\section{Data}
\label{sec:data}

\subsection{Data description}
The data consists of responses by women in United States to the National Survey of Family Growth carried out by the United States Center for Disease Control and Prevention. Questions in the survey span topics such as demographics, marriage and reproductive health. There are a total of 14,644 observations, with each observation representing an individual’s responses for one release of the survey (an individual may have participated in multiple rounds of the survey), and a total of 1378 features, including the survey round and 116 numeric (standardized), 1050 categorical and 211 ordinal features that are responses to survey questions. The data has been obfuscated, so it is not given what each feature corresponds to in real terms.

An overview of the survey data is given in Table~\ref{tab-data}.

\begin{table}[ht]
  \centering
  \begin{tabular}{@{}|l|c|c|c|c|@{}}
  	\hline
		& Male & Female & Missing & Total\\ 
		\hline
		All observations & 62 & 46 & 9 & 117 \\ 
			Excluding fraternity observations & 41 & 45 & 8 & 94\\ 
		\hline
	\end{tabular}
	\caption{\textbf{Summary of the survey observations}} 
  \label{tab-data}
\end{table}

\subsection{Data problems}
The data is very sparse, with 83\% of the data missing in the train set. There are many features with a high proportion of missing values as shown in Fig~\ref{fig-missing}. This is because are many missing values as some survey questions depend on the response to previous survey questions and may be skipped.

\begin{figure}[htbp]
    \begin{center}
		\includegraphics[width = 0.4\textwidth]{fig/"prop-missing-in-columns".pdf}
		\caption{\textbf{Proportion of missing values in features.}}
		\label{fig-missing}
  \end{center}
\end{figure}

Secondly, columns of predictors/whether a women goes to a healthcare provider for different services may be dependent on each other. Fitting separate independent models for each health care service may result in a loss of information.

\subsection{Data processing}
Features that contain only missing values or one unique value (i.e. constant value for all women) were dropped from the data. There were 14 and 20 of such features respectively.
%==================================================
\section{Methods}
\label{sec:methods}
\subsection{Feature engineering}
We engineered a number of features, including:
\begin{itemize}
  \item Number of numeric features with missing values for each woman
  \item Number of ordinal features with missing values for each woman
  \item Number of categorical features with missing values for each woman
\end{itemize}

\subsection{Models}
We fitted a variety of models, including:
\begin{itemize}
  \item Gradient Boosting Machine (GBM)
\end{itemize}

\subsection{Computation}
All calculations were carried out in the \texttt{R} programming language, with additional functions from the \texttt{caret} package. Annotated code is given in the Appendix in Section \ref{sec:appendix}.
%==================================================
\section{Results} 
\label{sec:results}

\subsection{Prediction objective and evaluation metric}
Binary outcomes denoting if a woman went to a healthcare provider for each of 14 services in the 12 months preceding the survey are given in the training set. The prediction objective is to predict the probability for each of the 3,661 women in the test set of going to a healthcare provider for each of the 14 services. In other words, 3,661 x 14 predicted probabilities are required. The 14 services are not mutually exclusive; a woman can go to more than one service in the 12 months.

The evaluation metric here is the logarithmic loss, defined by $-\frac{1}{n}$. (FILL IN!)

\subsection{Prediction results}
A summary of the performance of our models on the test set is given in Table~\ref{tab-results}.

\begin{table}[ht]
  \centering
  \begin{tabular}{@{}|l|c|c|@{}}
    \hline
  	Data set & Model & Log loss\\ 
		\hline
  	Missing value cut-off of 50\% & Gradient boosting method & 0.2796 \\ 
    Missing value cut-off of 80\% & Gradient boosting method & 0.2812 \\
  	\hline
	\end{tabular}
	\caption{\textbf{Summary of model performance on test set.}}
  \label{tab-results}
\end{table}
%==================================================
\section{Discussion} 
\label{sec:discussion}
%==================================================
\section{References}
\clearpage
%==================================================
\appendix
\section*{Appendix}
\label{sec:appendix}


\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{rm}\hlstd{(}\hlkwc{list} \hlstd{=} \hlkwd{ls}\hlstd{())}
\hlkwd{gc}\hlstd{()}
\hlstd{run_on_server} \hlkwb{<-} \hlnum{TRUE}
\hlkwa{if} \hlstd{(}\hlopt{!}\hlstd{run_on_server)}
  \hlkwd{setwd}\hlstd{(}\hlstr{"~/Copy/Berkeley/stat222-spring-2015/stat222sp15/projects/countable-care"}\hlstd{)}
\hlstd{data_dir} \hlkwb{<-} \hlstr{"data"}
\hlstd{fig_dir} \hlkwb{<-} \hlstr{"fig"}
\hlstd{results_dir} \hlkwb{<-} \hlstr{"results"}
\hlkwd{dir.create}\hlstd{(fig_dir,} \hlkwc{showWarnings} \hlstd{=} \hlnum{FALSE}\hlstd{)}
\hlkwd{dir.create}\hlstd{(results_dir,} \hlkwc{showWarnings} \hlstd{=} \hlnum{FALSE}\hlstd{)}
\hlkwd{dir.create}\hlstd{(}\hlstr{"submit"}\hlstd{,} \hlkwc{showWarnings} \hlstd{=} \hlnum{FALSE}\hlstd{)}
\hlstd{get_notifications} \hlkwb{<-} \hlkwd{ifelse}\hlstd{(run_on_server,} \hlnum{TRUE}\hlstd{,} \hlnum{FALSE}\hlstd{)}
\hlkwa{if} \hlstd{(get_notifications) \{}
  \hlkwd{library}\hlstd{(RPushbullet)}
  \hlcom{# options(error = function() \{ # Be notified when there is an error}
  \hlcom{#   pbPost("note", "Error!", geterrmessage(), recipients = c(1, 2))}
  \hlcom{# \})}
\hlstd{\}}

\hlstd{write_submission} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{probs}\hlstd{,} \hlkwc{model_name}\hlstd{) \{}
  \hlstd{file_path} \hlkwb{<-} \hlkwd{file.path}\hlstd{(}\hlstr{"submit"}\hlstd{,} \hlkwd{paste0}\hlstd{(model_name,} \hlstr{".csv"}\hlstd{))}
  \hlstd{submit} \hlkwb{<-} \hlkwd{read.csv}\hlstd{(}\hlstr{"data/SubmissionFormat.csv"}\hlstd{)}
  \hlstd{submit[,} \hlnum{2}\hlopt{:}\hlkwd{ncol}\hlstd{(submit)]} \hlkwb{<-} \hlstd{probs}
  \hlkwa{if} \hlstd{(}\hlkwd{file.exists}\hlstd{(file_path))}
    \hlkwd{stop}\hlstd{(}\hlkwd{paste0}\hlstd{(file_path,} \hlstr{" already exists!"}\hlstd{))}
  \hlkwd{write.csv}\hlstd{(submit,} \hlkwd{file.path}\hlstd{(file_path),} \hlkwc{row.names} \hlstd{=} \hlnum{FALSE}\hlstd{)}
  \hlkwd{message}\hlstd{(}\hlkwd{paste0}\hlstd{(}\hlstr{"Results written to "}\hlstd{, file_path))}
\hlstd{\}}

\hlstd{seed} \hlkwb{<-} \hlnum{12345}
\hlkwd{set.seed}\hlstd{(seed)}
\hlkwd{library}\hlstd{(caret)}
\hlkwd{library}\hlstd{(e1071)}
\hlcom{# List all models in caret}
\hlcom{# names(getModelInfo())}

\hlcom{# Load data}
\hlstd{prop_missing_cutoff} \hlkwb{<-} \hlnum{0.5}
\hlkwd{load}\hlstd{(}\hlkwc{file} \hlstd{=} \hlkwd{file.path}\hlstd{(data_dir,} \hlkwd{paste0}\hlstd{(}\hlstr{"data_cutoff"}\hlstd{, prop_missing_cutoff,} \hlstr{".rda"}\hlstd{)))}
\hlstd{train} \hlkwb{<-} \hlstd{data}\hlopt{$}\hlstd{train}
\hlstd{test} \hlkwb{<-} \hlstd{data}\hlopt{$}\hlstd{test}
\hlstd{ytrain} \hlkwb{<-} \hlstd{data}\hlopt{$}\hlstd{ytrain}

\hlcom{# Create data partitions of 80% and 20%}
\hlstd{ntrain} \hlkwb{<-} \hlkwd{nrow}\hlstd{(train)}
\hlstd{train_indices} \hlkwb{<-} \hlkwd{sample}\hlstd{(}\hlnum{1}\hlopt{:}\hlstd{ntrain)[}\hlnum{1}\hlopt{:}\hlkwd{floor}\hlstd{(ntrain}\hlopt{*}\hlnum{0.8}\hlstd{)]}
\hlstd{train_val} \hlkwb{<-} \hlstd{train[}\hlopt{-}\hlstd{train_indices, ]}

\hlcom{# Set up caret models}
\hlstd{train_control} \hlkwb{<-} \hlkwd{trainControl}\hlstd{(}\hlkwc{method} \hlstd{=} \hlstr{"cv"}\hlstd{,} \hlkwc{number} \hlstd{=} \hlnum{10}\hlstd{,} \hlkwc{returnResamp} \hlstd{=} \hlstr{"none"}\hlstd{)}

\hlstd{mod_types} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlstr{"gbm"}\hlstd{,} \hlstr{"rf"}\hlstd{)}
\hlstd{mod} \hlkwb{<-} \hlkwd{list}\hlstd{()}
\hlstd{probs} \hlkwb{<-} \hlkwd{matrix}\hlstd{(}\hlnum{NA}\hlstd{,} \hlkwd{nrow}\hlstd{(test),} \hlkwd{ncol}\hlstd{(ytrain))}
\hlkwa{for} \hlstd{(mod_type} \hlkwa{in} \hlstd{mod_types) \{}
  \hlkwa{for} \hlstd{(svc_index} \hlkwa{in} \hlnum{1}\hlopt{:}\hlkwd{ncol}\hlstd{(ytrain)) \{}

    \hlcom{# Testing!!!}
    \hlcom{# mod_type <- "knn"}
    \hlcom{# train_indices <- 1:100}
    \hlcom{# svc_index <- 1}

    \hlcom{# Train all the models with train data}
    \hlstd{mod[[svc_index]]} \hlkwb{<-} \hlkwd{train}\hlstd{(train[train_indices, ], ytrain[train_indices, svc_index],}
                              \hlkwc{method} \hlstd{= mod_type,} \hlkwc{trControl} \hlstd{= train_control)}

    \hlcom{# Predict on test data}
    \hlstd{probs[, svc_index]} \hlkwb{<-} \hlkwd{predict}\hlstd{(}\hlkwc{object} \hlstd{= mod[[svc_index]],} \hlkwc{newdata} \hlstd{= test,}
                                  \hlkwc{type} \hlstd{=} \hlstr{"prob"}\hlstd{)}\hlopt{$}\hlstd{yes}

    \hlcom{# Get predictions for each model and add them back to themselves}
    \hlcom{# train_val[[paste0(mod_type, "_PROB"]] <- predict(mod[[svc_index]], train_val, type = "prob")}
    \hlcom{# test[[paste0(mod_type, "_PROB"]] <- predict(mod[[svc_index]], test, type = "prob")}

    \hlcom{# Run an ensemble model to blend all the predicted probabilities}
    \hlcom{# mod_ensemble[[svc_index]] <- train(train_val, ytrain[-train_indices, svc_index], }
    \hlcom{#                                    method = "lasso", trControl = train_control)}

    \hlcom{# Predict on test data}
    \hlcom{# preds <- predict(mod_ensemble[[svc_index]], test, type = "prob")}
  \hlstd{\}}
  \hlkwd{write_submission}\hlstd{(probs,} \hlkwd{paste0}\hlstd{(mod_type,} \hlstr{"_cutoff"}\hlstd{, prop_missing_cutoff))}
  \hlkwd{save}\hlstd{(mod,} \hlkwc{file} \hlstd{=} \hlkwd{file.path}\hlstd{(results_dir,}
                             \hlkwd{paste0}\hlstd{(}\hlstr{"mod_"}\hlstd{, mod_type,} \hlstr{"_cutoff"}\hlstd{, prop_missing_cutoff,} \hlstr{".rda"}\hlstd{)))}
  \hlkwa{if} \hlstd{(get_notifications)}
    \hlkwd{pbPost}\hlstd{(}\hlkwc{type} \hlstd{=} \hlstr{"note"}\hlstd{,}
           \hlkwc{title} \hlstd{=} \hlstr{"stat222"}\hlstd{,}
           \hlkwc{body} \hlstd{=} \hlkwd{paste0}\hlstd{(mod_type,} \hlstr{" done!"}\hlstd{),}
           \hlkwc{recipients} \hlstd{=} \hlkwd{c}\hlstd{(}\hlnum{1}\hlstd{,} \hlnum{2}\hlstd{))}
\hlstd{\}}
\end{alltt}
\end{kframe}
\end{knitrout}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{rm}\hlstd{(}\hlkwc{list} \hlstd{=} \hlkwd{ls}\hlstd{())}
\hlkwd{gc}\hlstd{()}
\hlkwd{setwd}\hlstd{(}\hlstr{"~/Copy/Berkeley/stat222-spring-2015/stat222sp15/projects/countable-care"}\hlstd{)}
\hlstd{data_dir} \hlkwb{<-} \hlstr{"data"}
\hlstd{fig_dir} \hlkwb{<-} \hlstr{"fig"}
\hlstd{results_dir} \hlkwb{<-} \hlstr{"results"}
\hlkwd{dir.create}\hlstd{(fig_dir,} \hlkwc{showWarnings} \hlstd{=} \hlnum{FALSE}\hlstd{)}
\hlkwd{dir.create}\hlstd{(results_dir,} \hlkwc{showWarnings} \hlstd{=} \hlnum{FALSE}\hlstd{)}
\hlkwd{dir.create}\hlstd{(}\hlstr{"submit"}\hlstd{,} \hlkwc{showWarnings} \hlstd{=} \hlnum{FALSE}\hlstd{)}
\hlstd{prop_missing_cutoff} \hlkwb{<-} \hlnum{0.8}

\hlcom{# Read in data}
\hlstd{train_readin} \hlkwb{<-} \hlkwd{read.csv}\hlstd{(}\hlkwd{file.path}\hlstd{(data_dir,} \hlstr{"train_values.csv"}\hlstd{),}
                         \hlkwc{stringsAsFactors} \hlstd{=} \hlnum{FALSE}\hlstd{,} \hlkwc{na.strings} \hlstd{=} \hlstr{""}\hlstd{)}
\hlstd{ytrain} \hlkwb{<-} \hlkwd{read.csv}\hlstd{(}\hlkwd{file.path}\hlstd{(data_dir,} \hlstr{"train_labels.csv"}\hlstd{))}
\hlstd{test_readin} \hlkwb{<-} \hlkwd{read.csv}\hlstd{(}\hlkwd{file.path}\hlstd{(data_dir,} \hlstr{"test_values.csv"}\hlstd{),}
                        \hlkwc{stringsAsFactors} \hlstd{=} \hlnum{FALSE}\hlstd{,} \hlkwc{na.strings} \hlstd{=} \hlstr{""}\hlstd{)}

\hlcom{# Data exploration}
\hlcom{# Column types}
\hlstd{colnames_all} \hlkwb{<-} \hlkwd{names}\hlstd{(train_readin)}
\hlstd{colnames_type} \hlkwb{<-} \hlkwd{sapply}\hlstd{(colnames_all,} \hlkwa{function}\hlstd{(}\hlkwc{x}\hlstd{)} \hlkwd{strsplit}\hlstd{(x,} \hlstr{"_"}\hlstd{)[[}\hlnum{1}\hlstd{]][}\hlnum{1}\hlstd{])}
\hlkwd{table}\hlstd{(colnames_type[}\hlopt{-}\hlkwd{c}\hlstd{(}\hlnum{1}\hlstd{,} \hlnum{2}\hlstd{)])}

\hlcom{# Check proportion of missing values in features}
\hlstd{prop_missing} \hlkwb{<-} \hlkwd{sapply}\hlstd{(train_readin,} \hlkwa{function}\hlstd{(}\hlkwc{x}\hlstd{)} \hlkwd{mean}\hlstd{(}\hlkwd{is.na}\hlstd{(x)))}
\hlkwd{par}\hlstd{(}\hlkwc{mar} \hlstd{=} \hlkwd{c}\hlstd{(}\hlnum{4.5}\hlstd{,} \hlnum{4.5}\hlstd{,} \hlnum{4.5}\hlstd{,} \hlnum{2}\hlstd{))}
\hlkwd{pdf}\hlstd{(}\hlkwd{file.path}\hlstd{(fig_dir,} \hlstr{"prop-missing-in-columns.pdf"}\hlstd{),} \hlkwc{width} \hlstd{=} \hlnum{7}\hlstd{,} \hlkwc{height} \hlstd{=} \hlnum{5}\hlstd{)}
\hlkwd{hist}\hlstd{(prop_missing,} \hlkwc{main} \hlstd{=} \hlstr{"Proportion of missing values\textbackslash{}nin features"}\hlstd{,}
     \hlkwc{xlab} \hlstd{=} \hlstr{"Proportion"}\hlstd{,} \hlkwc{ylab} \hlstd{=} \hlstr{"Number of features"}\hlstd{,} \hlkwc{col} \hlstd{=} \hlstr{"lightgrey"}\hlstd{)}
\hlkwd{dev.off}\hlstd{()}

\hlcom{# Missing pattern plot}
\hlcom{# library(mi)}
\hlcom{# missing.pattern.plot(train)}

\hlcom{# Data processing}
\hlcom{# Check original number of features, not including id}
\hlkwd{ncol}\hlstd{(train_readin[,} \hlopt{-}\hlnum{1}\hlstd{])} \hlcom{# 1378}

\hlcom{# Check for features with only constant values}
\hlstd{cols_constant} \hlkwb{<-} \hlkwd{sapply}\hlstd{(train_readin,} \hlkwa{function}\hlstd{(}\hlkwc{x}\hlstd{)} \hlkwd{length}\hlstd{(}\hlkwd{unique}\hlstd{(x))} \hlopt{==} \hlnum{1}\hlstd{)}
\hlkwd{sum}\hlstd{(cols_constant)} \hlcom{# 20}
\hlcom{# names(train_readin)[cols_constant]}

\hlcom{# Feature engineering}
\hlcom{# Column types}
\hlstd{colnames_all} \hlkwb{<-} \hlkwd{names}\hlstd{(train_readin)}
\hlstd{colnames_type} \hlkwb{<-} \hlkwd{sapply}\hlstd{(colnames_all,} \hlkwa{function}\hlstd{(}\hlkwc{x}\hlstd{)} \hlkwd{strsplit}\hlstd{(x,} \hlstr{"_"}\hlstd{)[[}\hlnum{1}\hlstd{]][}\hlnum{1}\hlstd{])}
\hlkwd{table}\hlstd{(colnames_type[}\hlopt{-}\hlkwd{c}\hlstd{(}\hlnum{1}\hlstd{,} \hlnum{2}\hlstd{)])}
\hlstd{cols_numeric} \hlkwb{<-} \hlstd{colnames_type} \hlopt{==} \hlstr{"n"}
\hlstd{cols_ordinal} \hlkwb{<-} \hlstd{colnames_type} \hlopt{==} \hlstr{"o"}
\hlstd{cols_categorical} \hlkwb{<-} \hlstd{colnames_type} \hlopt{==} \hlstr{"c"}

\hlcom{# Number of missing numeric/ordinal/categorical features}
\hlstd{num_missing_numeric} \hlkwb{<-} \hlkwd{apply}\hlstd{(train_readin[, cols_numeric],} \hlnum{1}\hlstd{,} \hlkwa{function}\hlstd{(}\hlkwc{x}\hlstd{)} \hlkwd{sum}\hlstd{(}\hlkwd{is.na}\hlstd{(x)))}
\hlstd{num_missing_ordinal} \hlkwb{<-} \hlkwd{apply}\hlstd{(train_readin[, cols_ordinal],} \hlnum{1}\hlstd{,} \hlkwa{function}\hlstd{(}\hlkwc{x}\hlstd{)} \hlkwd{sum}\hlstd{(}\hlkwd{is.na}\hlstd{(x)))}
\hlstd{num_missing_categorical} \hlkwb{<-} \hlkwd{apply}\hlstd{(train_readin[, cols_categorical],} \hlnum{1}\hlstd{,} \hlkwa{function}\hlstd{(}\hlkwc{x}\hlstd{)} \hlkwd{sum}\hlstd{(}\hlkwd{is.na}\hlstd{(x)))}
\hlkwd{hist}\hlstd{(num_missing_numeric,} \hlkwc{freq} \hlstd{=} \hlnum{FALSE}\hlstd{,}
     \hlkwc{main} \hlstd{=} \hlstr{"Distribution of missing\textbackslash{}nnumeric features across all women"}\hlstd{,}
     \hlkwc{xlab} \hlstd{=} \hlstr{"Number of missing numeric features"}\hlstd{)}
\hlkwd{hist}\hlstd{(num_missing_ordinal,} \hlkwc{freq} \hlstd{=} \hlnum{FALSE}\hlstd{,}
     \hlkwc{main} \hlstd{=} \hlstr{"Distribution of missing\textbackslash{}nordinal features across all women"}\hlstd{,}
     \hlkwc{xlab} \hlstd{=} \hlstr{"Number of missing ordinal features"}\hlstd{)}
\hlkwd{hist}\hlstd{(num_missing_categorical,} \hlkwc{freq} \hlstd{=} \hlnum{FALSE}\hlstd{,}
     \hlkwc{main} \hlstd{=} \hlstr{"Distribution of missing\textbackslash{}ncategorical features across all women"}\hlstd{,}
     \hlkwc{xlab} \hlstd{=} \hlstr{"Number of missing categorical features"}\hlstd{)}
\hlstd{num_missing_numeric_test} \hlkwb{<-} \hlkwd{apply}\hlstd{(test_readin[, cols_numeric],} \hlnum{1}\hlstd{,} \hlkwa{function}\hlstd{(}\hlkwc{x}\hlstd{)} \hlkwd{sum}\hlstd{(}\hlkwd{is.na}\hlstd{(x)))}
\hlstd{num_missing_ordinal_test} \hlkwb{<-} \hlkwd{apply}\hlstd{(test_readin[, cols_ordinal],} \hlnum{1}\hlstd{,} \hlkwa{function}\hlstd{(}\hlkwc{x}\hlstd{)} \hlkwd{sum}\hlstd{(}\hlkwd{is.na}\hlstd{(x)))}
\hlstd{num_missing_categorical_test} \hlkwb{<-} \hlkwd{apply}\hlstd{(test_readin[, cols_categorical],} \hlnum{1}\hlstd{,} \hlkwa{function}\hlstd{(}\hlkwc{x}\hlstd{)} \hlkwd{sum}\hlstd{(}\hlkwd{is.na}\hlstd{(x)))}

\hlcom{# Check for features with proportion of missing values > prop_missing_cutoff}
\hlstd{prop_missing} \hlkwb{<-} \hlkwd{sapply}\hlstd{(train_readin,} \hlkwa{function}\hlstd{(}\hlkwc{x}\hlstd{)} \hlkwd{mean}\hlstd{(}\hlkwd{is.na}\hlstd{(x)))}
\hlstd{cols_missing} \hlkwb{<-} \hlstd{prop_missing} \hlopt{>} \hlstd{prop_missing_cutoff}
\hlkwd{sum}\hlstd{(cols_missing)} \hlcom{# 1159 for 0.5, 1038 for 0.8}

\hlcom{# Remaining number of features, not including id}
\hlkwd{sum}\hlstd{(}\hlopt{!}\hlstd{cols_constant} \hlopt{& !}\hlstd{cols_missing)} \hlopt{-} \hlnum{1} \hlcom{# 213 for 0.5, 334 for 0.8}

\hlcom{# Remove above features and id}
\hlstd{train} \hlkwb{<-} \hlstd{train_readin[,} \hlkwd{names}\hlstd{(train_readin)} \hlopt{!=} \hlstr{"id"} \hlopt{& !}\hlstd{cols_constant} \hlopt{& !}\hlstd{cols_missing]}
\hlstd{test} \hlkwb{<-} \hlstd{test_readin[,} \hlkwd{names}\hlstd{(train_readin)} \hlopt{!=} \hlstr{"id"} \hlopt{& !}\hlstd{cols_constant} \hlopt{& !}\hlstd{cols_missing]}

\hlcom{# Column types}
\hlstd{colnames_all} \hlkwb{<-} \hlkwd{names}\hlstd{(train)}
\hlstd{colnames_type} \hlkwb{<-} \hlkwd{sapply}\hlstd{(colnames_all,} \hlkwa{function}\hlstd{(}\hlkwc{x}\hlstd{)} \hlkwd{strsplit}\hlstd{(x,} \hlstr{"_"}\hlstd{)[[}\hlnum{1}\hlstd{]][}\hlnum{1}\hlstd{])}
\hlkwd{table}\hlstd{(colnames_type[}\hlopt{-}\hlkwd{c}\hlstd{(}\hlnum{1}\hlstd{,} \hlnum{2}\hlstd{)])}
\hlstd{cols_numeric} \hlkwb{<-} \hlstd{colnames_type} \hlopt{==} \hlstr{"n"}
\hlstd{cols_ordinal} \hlkwb{<-} \hlstd{colnames_type} \hlopt{==} \hlstr{"o"}
\hlstd{cols_categorical} \hlkwb{<-} \hlstd{colnames_type} \hlopt{==} \hlstr{"c"}

\hlcom{# Missing value imputation for remaining features}
\hlcom{# Numeric features: Set as 0}
\hlkwa{for} \hlstd{(i} \hlkwa{in} \hlnum{1}\hlopt{:}\hlkwd{sum}\hlstd{(cols_numeric)) \{}
  \hlstd{train[, cols_numeric][, i]} \hlkwb{<-} \hlkwd{ifelse}\hlstd{(}\hlkwd{is.na}\hlstd{(train[, cols_numeric][, i]),}
                                       \hlnum{0}\hlstd{, train[, cols_numeric][, i])}
  \hlstd{test[, cols_numeric][, i]} \hlkwb{<-} \hlkwd{ifelse}\hlstd{(}\hlkwd{is.na}\hlstd{(test[, cols_numeric][, i]),}
                                      \hlnum{0}\hlstd{, test[, cols_numeric][, i])}
\hlstd{\}}
\hlcom{# Ordinal features: Set as -1}
\hlcom{# table(sapply(train[, cols_ordinal], min, na.rm = TRUE)) # min is 0 or 1}
\hlkwa{for} \hlstd{(i} \hlkwa{in} \hlnum{1}\hlopt{:}\hlkwd{sum}\hlstd{(cols_ordinal)) \{}
  \hlstd{train[, cols_ordinal][, i]} \hlkwb{<-} \hlkwd{as.integer}\hlstd{(}\hlkwd{ifelse}\hlstd{(}\hlkwd{is.na}\hlstd{(train[, cols_ordinal][, i]),}
                                              \hlopt{-}\hlnum{1}\hlstd{, train[, cols_ordinal][, i]))}
  \hlstd{test[, cols_ordinal][, i]} \hlkwb{<-} \hlkwd{as.integer}\hlstd{(}\hlkwd{ifelse}\hlstd{(}\hlkwd{is.na}\hlstd{(test[, cols_ordinal][, i]),}
                                             \hlopt{-}\hlnum{1}\hlstd{, test[, cols_ordinal][, i]))}
\hlstd{\}}

\hlcom{# Categorical features}
\hlcom{# Check for: i) features with categories in test but not train set.}
\hlcom{# ii) features with missing values in test but not train set}
\hlstd{cols_unknownlevels} \hlkwb{<-} \hlkwa{NULL}
\hlstd{cols_nomissingintrain} \hlkwb{<-} \hlkwa{NULL}
\hlkwa{for} \hlstd{(i} \hlkwa{in} \hlnum{1}\hlopt{:}\hlkwd{sum}\hlstd{(cols_categorical)) \{}
  \hlkwa{if} \hlstd{(}\hlkwd{any}\hlstd{(}\hlkwd{is.na}\hlstd{(test[, cols_categorical][, i]))} \hlopt{&} \hlkwd{all}\hlstd{(}\hlopt{!}\hlkwd{is.na}\hlstd{(train[, cols_categorical][, i]))) \{}
    \hlkwd{print}\hlstd{(i)}
    \hlstd{cols_nomissingintrain} \hlkwb{<-} \hlkwd{c}\hlstd{(cols_nomissingintrain, i)}
  \hlstd{\}}
  \hlkwa{if} \hlstd{(}\hlkwd{any}\hlstd{(}\hlopt{!}\hlkwd{is.na}\hlstd{(test[, cols_categorical][, i])} \hlopt{&}
            \hlopt{!}\hlstd{(test[, cols_categorical][, i]} \hlopt{%in%} \hlkwd{unique}\hlstd{(train[, cols_categorical][, i])))) \{}
    \hlstd{cols_unknownlevels} \hlkwb{<-} \hlkwd{c}\hlstd{(cols_unknownlevels, i)}
    \hlstd{levels_train} \hlkwb{<-} \hlkwd{unique}\hlstd{(train[, cols_categorical][, i])}
    \hlstd{levels_train} \hlkwb{<-} \hlkwd{ifelse}\hlstd{(}\hlkwd{is.na}\hlstd{(levels_train),} \hlstr{"missing"}\hlstd{,} \hlkwd{as.character}\hlstd{(levels_train))}
    \hlstd{levels_test} \hlkwb{<-} \hlkwd{unique}\hlstd{(test[, cols_categorical][, i])}
    \hlstd{levels_test} \hlkwb{<-} \hlkwd{ifelse}\hlstd{(}\hlkwd{is.na}\hlstd{(levels_test),} \hlstr{"missing"}\hlstd{,} \hlkwd{as.character}\hlstd{(levels_test))}
    \hlkwa{if} \hlstd{(}\hlopt{!}\hlstd{(}\hlstr{"missing"} \hlopt{%in%} \hlstd{levels_train)) \{}
      \hlkwd{print}\hlstd{(i)}
      \hlkwd{print}\hlstd{(levels_test[}\hlopt{!}\hlstd{(levels_test} \hlopt{%in%} \hlstd{levels_train)])}
      \hlkwd{print}\hlstd{(}\hlstr{"---"}\hlstd{)}
      \hlstd{cols_nomissingintrain} \hlkwb{<-} \hlkwd{c}\hlstd{(cols_nomissingintrain,}
                                 \hlkwd{rep}\hlstd{(i,} \hlkwd{length}\hlstd{(levels_test[}\hlopt{!}\hlstd{(levels_test} \hlopt{%in%} \hlstd{levels_train)])))}
    \hlstd{\}}
  \hlstd{\}}
\hlstd{\}}

\hlcom{# Categorical features: Set as new category missing}
\hlkwa{for} \hlstd{(i} \hlkwa{in} \hlnum{1}\hlopt{:}\hlkwd{sum}\hlstd{(cols_categorical)) \{}
  \hlstd{train[, cols_categorical][, i]} \hlkwb{<-} \hlkwd{as.factor}\hlstd{(}\hlkwd{ifelse}\hlstd{(}\hlkwd{is.na}\hlstd{(train[, cols_categorical][, i]),}
                                                  \hlstr{"missing"}\hlstd{, train[, cols_categorical][, i]))}
  \hlstd{test[, cols_categorical][, i]} \hlkwb{<-} \hlkwd{as.factor}\hlstd{(}\hlkwd{ifelse}\hlstd{(}\hlkwd{is.na}\hlstd{(test[, cols_categorical][, i]),}
                                                    \hlstr{"missing"}\hlstd{,}
                                                    \hlkwd{ifelse}\hlstd{(}\hlopt{!}\hlstd{(test[, cols_categorical][, i]} \hlopt{%in%}
                                                      \hlkwd{unique}\hlstd{(train[, cols_categorical][, i])),}
                                                      \hlstr{"missing"}\hlstd{, test[, cols_categorical][, i])))}
\hlstd{\}}

\hlcom{# Set values in test set to most frequently-occurring category in train set for:}
\hlcom{# i) features with categories in test but not train set.}
\hlcom{# ii) features with missing values in test but not train set}
\hlkwa{for} \hlstd{(c} \hlkwa{in} \hlkwd{seq_along}\hlstd{(cols_nomissingintrain)) \{}
  \hlstd{i} \hlkwb{<-} \hlstd{cols_nomissingintrain[c]}
  \hlstd{value_new} \hlkwb{<-} \hlkwd{names}\hlstd{(}\hlkwd{which.max}\hlstd{(}\hlkwd{table}\hlstd{(test[, cols_categorical][, i])))}
  \hlstd{test[, cols_categorical][, i]} \hlkwb{<-} \hlkwd{as.factor}\hlstd{(}\hlkwd{ifelse}\hlstd{(}\hlkwd{as.character}\hlstd{(test[, cols_categorical][, i])} \hlopt{==}
                                                      \hlstr{"missing"}\hlstd{,}
                                                    \hlstd{value_new,}
                                                    \hlkwd{as.character}\hlstd{(test[, cols_categorical][, i])))}
\hlstd{\}}

\hlcom{# Add engineered features to data}
\hlstd{train}\hlopt{$}\hlstd{num_missing_numeric} \hlkwb{<-} \hlstd{num_missing_numeric}
\hlstd{train}\hlopt{$}\hlstd{num_missing_ordinal} \hlkwb{<-} \hlstd{num_missing_ordinal}
\hlstd{train}\hlopt{$}\hlstd{num_missing_categorical} \hlkwb{<-} \hlstd{num_missing_categorical}
\hlstd{test}\hlopt{$}\hlstd{num_missing_numeric} \hlkwb{<-} \hlstd{num_missing_numeric_test}
\hlstd{test}\hlopt{$}\hlstd{num_missing_ordinal} \hlkwb{<-} \hlstd{num_missing_ordinal_test}
\hlstd{test}\hlopt{$}\hlstd{num_missing_categorical} \hlkwb{<-} \hlstd{num_missing_categorical_test}

\hlcom{# Convert release variable to factor, else it throws error}
\hlstd{train}\hlopt{$}\hlstd{release} \hlkwb{<-} \hlkwd{as.factor}\hlstd{(train}\hlopt{$}\hlstd{release)}
\hlstd{test}\hlopt{$}\hlstd{release} \hlkwb{<-} \hlkwd{as.factor}\hlstd{(test}\hlopt{$}\hlstd{release)}

\hlcom{# Convert prediction label to alphabetical factor, }
\hlcom{# else it throws error in caret::predict}
\hlkwa{for} \hlstd{(i} \hlkwa{in} \hlnum{1}\hlopt{:}\hlkwd{ncol}\hlstd{(ytrain))}
  \hlstd{ytrain[, i]} \hlkwb{<-} \hlkwd{factor}\hlstd{(}\hlkwd{ifelse}\hlstd{(ytrain[, i]} \hlopt{==} \hlnum{1}\hlstd{,} \hlstr{"yes"}\hlstd{,} \hlstr{"no"}\hlstd{))}

\hlcom{# Save processed data to file}
\hlstd{data} \hlkwb{<-} \hlkwd{list}\hlstd{(}\hlkwc{train} \hlstd{= train,}
             \hlkwc{ytrain} \hlstd{= ytrain[,} \hlopt{-}\hlnum{1}\hlstd{],} \hlcom{# Drop id column }
             \hlkwc{test} \hlstd{= test,}
             \hlkwc{prop_missing_cutoff} \hlstd{= prop_missing_cutoff)}
\hlkwd{save}\hlstd{(data,} \hlkwc{file} \hlstd{=} \hlkwd{file.path}\hlstd{(data_dir,} \hlkwd{paste0}\hlstd{(}\hlstr{"data_cutoff"}\hlstd{, prop_missing_cutoff,} \hlstr{".rda"}\hlstd{)))}
\end{alltt}
\end{kframe}
\end{knitrout}
\end{document}
